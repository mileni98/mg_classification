{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import findspark\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType, FloatType, IntegerType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, PCA\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, LinearSVC\n",
    "\n",
    "# Load environment variables from .env file\n",
    "_ = load_dotenv()\n",
    "\n",
    "# Retrieve environment variables\n",
    "TARGET_DIR_NAME = os.getenv(\"TARGET_DIR_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session running on 12 cores. UI is available at: http://172.20.0.1:4050\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "\n",
    "# Initialize Spark session to run locally\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Big Data Classification\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.ui.port\", \"4050\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Print the number of cores being used by Spark\n",
    "print(f\"Spark Session running on {spark.sparkContext.defaultParallelism} cores. UI is available at: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions for infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data: DataFrame) -> DataFrame:\n",
    "    \"\"\"Preprocess the data by removing rows with NaN values.\"\"\"    \n",
    "\n",
    "    # Remove rows with NaN values and count the number of rows before and after\n",
    "    total_rows_before = data.count()\n",
    "    clean_data = data.dropna()\n",
    "    total_rows_after = clean_data.count()\n",
    "\n",
    "    if total_rows_before != total_rows_after:\n",
    "        print(f\"Number of rows with NaN values: {total_rows_before - total_rows_after}\")\n",
    "\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_patient_overlap(train_df: DataFrame, test_df: DataFrame, patient_column: str = 'patient', verbose_just_error: bool = False) -> None:\n",
    "    \"\"\"Checks for overlapping patients between train and test DataFrames.\"\"\"\n",
    "\n",
    "    # Get unique patients from train and test DataFrames\n",
    "    train_patients = train_df.select(patient_column).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    test_patients = test_df.select(patient_column).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    # Convert lists to sets and check for any overlap\n",
    "    overlapping_patients = set(train_patients).intersection(set(test_patients))\n",
    "\n",
    "    # Print overlapping patients, if any\n",
    "    if overlapping_patients:\n",
    "        print(f\"Overlapping patients found: {overlapping_patients}\")\n",
    "    elif not verbose_just_error:\n",
    "        print(\"No overlapping patients between training and test sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_distribution(df1: DataFrame, df2: DataFrame, class_column: str = 'class') -> None:\n",
    "    \"\"\"Calculates and prints class distribution percentages for two DataFrames in a single line.\"\"\"\n",
    "    \n",
    "    # Calculate class counts for both DataFrames\n",
    "    df1_counts = df1.groupBy(class_column).count().orderBy(class_column).collect()\n",
    "    df2_counts = df2.groupBy(class_column).count().orderBy(class_column).collect()\n",
    "    \n",
    "    # Calculate total counts\n",
    "    total_count_df1 = df1.count()\n",
    "    total_count_df2 = df2.count()\n",
    "    \n",
    "    # Prepare class distributions as strings\n",
    "    df1_distribution = ', '.join([f\"{row[class_column]}: {row['count']} ({(row['count'] / total_count_df1) * 100:.2f}%)\" for row in df1_counts])\n",
    "    df2_distribution = ', '.join([f\"{row[class_column]}: {row['count']} ({(row['count'] / total_count_df2) * 100:.2f}%)\" for row in df2_counts])\n",
    "    \n",
    "    # Print class distributions in one line\n",
    "    print(f\"Class distribution in train_df: [{df1_distribution}] | test_df: [{df2_distribution}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_by_patient(data: DataFrame, test_ratio: float = 0.2, patient_column: str = 'patient', seed: int = 42) -> tuple[DataFrame, DataFrame]:\n",
    "    \"\"\" Splits the DataFrame into train and test sets based on unique patient IDs, ensuring no patient data overlap.\"\"\"\n",
    "\n",
    "    random.seed(seed) # Set seed for reproducibility\n",
    "    \n",
    "    # Extract unique patient IDs and randomly shuffle them\n",
    "    unique_patients = data.select(patient_column).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    random.shuffle(unique_patients)\n",
    "    \n",
    "    # Split the patients into train and test sets\n",
    "    split_index = int(len(unique_patients) * (1 - test_ratio))\n",
    "    train_patients = unique_patients[:split_index]\n",
    "    test_patients = unique_patients[split_index:]\n",
    "    \n",
    "    # Create train and test DataFrames \n",
    "    train_df = data.filter(col(patient_column).isin(train_patients))\n",
    "    test_df = data.filter(col(patient_column).isin(test_patients))\n",
    "    check_patient_overlap(train_df, test_df)\n",
    "    \n",
    "    # Calculate and print the percentages of train and test sets\n",
    "    total_rows, train_rows, test_rows = data.count(), train_df.count(), test_df.count()\n",
    "    print(f\"Training size: {train_rows} ({(train_rows / total_rows) * 100:.2f}%), Test size: {test_rows} rows ({(test_rows / total_rows) * 100:.2f}%)\") \n",
    "    \n",
    "    calculate_class_distribution(train_df, test_df)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds_by_patient(data: DataFrame, num_folds: int = 5, patient_column: str = 'patient', seed: int = 42) -> list:\n",
    "    \"\"\"Splits the data into train-test folds based on unique patient IDs, ensuring no patient overlap across folds.\"\"\"\n",
    "\n",
    "    random.seed(seed) # Set seed for reproducibility\n",
    "\n",
    "    # Extract unique patient IDs and randomly shuffle them\n",
    "    unique_patients = data.select(patient_column).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    random.shuffle(unique_patients)\n",
    "\n",
    "    # Split patients evenly into folds\n",
    "    fold_size = len(unique_patients) // num_folds\n",
    "    folds = [unique_patients[i * fold_size:(i + 1) * fold_size] for i in range(num_folds)]\n",
    "\n",
    "    # Handle any remaining patients by adding them to the last fold\n",
    "    for i in range(len(unique_patients) % num_folds):\n",
    "        folds[i].append(unique_patients[-(i + 1)])\n",
    "\n",
    "    patient_folds, fold_summaries = [], []\n",
    "\n",
    "    # Create train-test splits for each fold\n",
    "    total_rows = data.count()\n",
    "    for i in range(num_folds):\n",
    "\n",
    "        # Get the test patients for the current fold and assign the rest to the training set\n",
    "        test_patients = set(folds[i])\n",
    "        train_patients = set(unique_patients) - test_patients\n",
    "\n",
    "        # Filter the data based on the train and test patients\n",
    "        train_df = data.filter(col(patient_column).isin(train_patients))\n",
    "        test_df = data.filter(col(patient_column).isin(test_patients))\n",
    "        check_patient_overlap(train_df, test_df, verbose_just_error=True)\n",
    "\n",
    "        # Append the train-test split to the list of folds\n",
    "        patient_folds.append((train_df, test_df))\n",
    "\n",
    "        # Accumulate test set summary for this fold\n",
    "        fold_summaries.append(f\"Fold {i + 1}: {test_df.count()} rows ({(test_df.count() / total_rows) * 100:.2f}%)\")\n",
    "\n",
    "    print(\" | \".join(fold_summaries))\n",
    "\n",
    "    # Return the list of train-test tuples for each fold\n",
    "    return patient_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(data: DataFrame, output_column: str = 'pca_features', k: int = 10) -> DataFrame:\n",
    "    \"\"\"Applies PCA to reduce the dimensionality of the feature set to k principal components.\"\"\"\n",
    "    \n",
    "    # Initialize PCA with the specified number of components\n",
    "    pca = PCA(k=k, inputCol='features', outputCol=output_column)\n",
    "    \n",
    "    # Fit PCA on the data and transform the features\n",
    "    pca_model = pca.fit(data)\n",
    "    transformed_data = pca_model.transform(data)\n",
    "    \n",
    "    print(f\"PCA explained variance ratio: {pca_model.explainedVariance.toArray()}\")\n",
    "    \n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_features(data: DataFrame, class_column: str = 'class', patient_column: str = 'patient', output_column: str = 'features', verbose: bool = True) -> DataFrame:\n",
    "    \"\"\"Assembles feature columns into a single feature vector column, excluding patient and class columns.\"\"\"\n",
    "    \n",
    "    # Get the numeric columns from the DataFrame\n",
    "    numeric_columns = [field.name for field in data.schema.fields if isinstance(field.dataType, (DoubleType, FloatType, IntegerType))]\n",
    "\n",
    "    # Define feature columns by excluding the numeric columns and the class and patient columns\n",
    "    feature_columns = [col for col in numeric_columns if col not in [class_column, patient_column]]\n",
    "    if verbose:\n",
    "        print(f\"Number of initial columns: {len(data.columns)}, number of feature columns: {len(feature_columns)}\")\n",
    "    \n",
    "    # Assemble features into a feature vector\n",
    "    assembler = VectorAssembler(inputCols = feature_columns, outputCol = output_column)\n",
    "    assembled_data = assembler.transform(data)\n",
    "    \n",
    "    return assembled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions: DataFrame) -> tuple:\n",
    "    \"\"\"Calculates accuracy, precision, and recall from the predictions DataFrame.\"\"\"\n",
    "\n",
    "    # Calculate true positives, true negatives, false positives, and false negatives\n",
    "    tp = predictions.filter((col('prediction') == 1) & (col('class') == 1)).count()\n",
    "    tn = predictions.filter((col('prediction') == 0) & (col('class') == 0)).count()\n",
    "    fp = predictions.filter((col('prediction') == 1) & (col('class') == 0)).count()\n",
    "    fn = predictions.filter((col('prediction') == 0) & (col('class') == 1)).count()\n",
    "\n",
    "    # Calculate accuracy, precision, and recall\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_best_parameters_on_folds(model, train_df: DataFrame, paramGrid: list[dict], num_folds: int = 5, patient_column: str = 'patient'):\n",
    "    \"\"\"Evaluates the Decision Tree model using patient-wise cross-validation with specified hyperparameter tuning.\"\"\"\n",
    "\n",
    "    folds = create_folds_by_patient(train_df, num_folds, patient_column)\n",
    "\n",
    "    # Use BinaryClassificationEvaluator to measure performance\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol = \"class\", rawPredictionCol = \"prediction\", metricName = \"areaUnderROC\")\n",
    "\n",
    "    best_params, best_metric = None, float(\"-inf\")\n",
    "\n",
    "    # Iterate over each parameter combination in the grid\n",
    "    for params in tqdm(paramGrid, desc=\"Hyperparameter Tuning\", leave = True):\n",
    "        fold_metrics = []\n",
    "\n",
    "        # For each fold, evaluate the parameter combination\n",
    "        for _, (train_df, test_df) in enumerate(folds):\n",
    "\n",
    "            # Vectorize features\n",
    "            train_df = vectorize_features(train_df, verbose = False)\n",
    "            test_df = vectorize_features(test_df, verbose = False)\n",
    "            check_patient_overlap(train_df, test_df, verbose_just_error = True)\n",
    "\n",
    "            # Train the model with the current parameters\n",
    "            current_model = model.copy(params).fit(train_df)\n",
    "\n",
    "            # Evaluate the model on the test data using the specified metric\n",
    "            predictions = current_model.transform(test_df)\n",
    "            metric = evaluator.evaluate(predictions)\n",
    "            fold_metrics.append(metric)\n",
    "        \n",
    "        # Calculate average metric across all folds for the current parameter combination\n",
    "        avg_metric = __builtins__.sum(fold_metrics) / len(fold_metrics)\n",
    "\n",
    "        # Update the best parameters if the current average metric is better\n",
    "        if avg_metric > best_metric:\n",
    "            best_metric = avg_metric\n",
    "            best_params = params\n",
    "\n",
    "    # Print the best parameters after evaluating all combinations\n",
    "    param_items = [f\"{param.name}: {value}\" for param, value in best_params.items()]\n",
    "    print(\"Best Overall Parameters:\", ', '.join(param_items))\n",
    "    print(f\"Best {evaluator.getMetricName()}: {best_metric:.4f}\")\n",
    "\n",
    "    return best_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_best_model(model, best_params: list[dict], train_df: DataFrame, test_df: DataFrame, is_tree: bool = False):\n",
    "    \"\"\"Train the model on the full training data using the best parameters and evaluate on the test data.\"\"\"\n",
    "\n",
    "    # Vectorize features\n",
    "    train_df = vectorize_features(train_df, verbose = False)\n",
    "    test_df = vectorize_features(test_df)\n",
    "\n",
    "    # Train the model using the best parameters\n",
    "    best_model = model.copy(best_params).fit(train_df)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"class\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "    predictions = best_model.transform(test_df)\n",
    "    metric = evaluator.evaluate(predictions)\n",
    "\n",
    "    # Calculate additional metrics: accuracy, precision, recall\n",
    "    accuracy, precision, recall = calculate_metrics(predictions)\n",
    "\n",
    "    print(f\"Final Model Evaluation on Test Data {evaluator.getMetricName()}: {metric:.4f}\")\n",
    "    print(f\"Final Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "\n",
    "    # Print feature importances if the model is a tree-based model\n",
    "    if is_tree:\n",
    "        # Extract the feature importances from the model\n",
    "        feature_importances = best_model.featureImportances.toArray()\n",
    "\n",
    "        # Get feature column names from the assembler\n",
    "        feature_columns = train_df.schema['features'].metadata['ml_attr']['attrs']['numeric']\n",
    "        feature_names = [attr['name'] for attr in feature_columns]\n",
    "\n",
    "        # Combine feature names and their importances, then sort by importance\n",
    "        sorted_features = sorted(zip(feature_names, feature_importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Print sorted feature importances\n",
    "        print(\"\\nSorted Feature Importances:\")\n",
    "        for name, importance in sorted_features:\n",
    "            print(f\"Feature: {name}, Importance: {importance:.4f}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model: str, file_name: str, paramGrid: list[dict], is_tree: bool = False):\n",
    "\n",
    "    # Load and preprocess the dataset\n",
    "    file_path = os.path.join(os.getcwd(), TARGET_DIR_NAME, file_name)\n",
    "    data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "    print(f\"\\n{'=' * 75}\\n Setting up\\n{'=' * 75}\\n\")\n",
    "    clean_data = preprocess_data(data)\n",
    "    clean_data = clean_data.limit(100)\n",
    "\n",
    "    # split the data into train and test sets\n",
    "    train_df, test_df = train_test_split_by_patient(clean_data, test_ratio=0.2)\n",
    "\n",
    "    # Evaluate the model using patient-wise cross-validation to find the best parameters\n",
    "    print(f\"\\n{'=' * 75}\\n Hyperparameter tunning\\n{'=' * 75}\\n\")\n",
    "    best_params = determine_best_parameters_on_folds(model, train_df, paramGrid, num_folds=5, patient_column='patient')\n",
    "\n",
    "    # Train the best model on the full training data and evaluate on the test data\n",
    "    print(f\"\\n{'=' * 75}\\n Testing model on test dataset\\n{'=' * 75}\\n\")\n",
    "    best_model = train_and_evaluate_best_model(model, best_params, train_df, test_df, is_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 78 (78.00%), Test size: 22 rows (22.00%)\n",
      "Class distribution in train_df: [0: 39 (50.00%), 1: 39 (50.00%)] | test_df: [0: 11 (50.00%), 1: 11 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 20 rows (25.64%) | Fold 2: 28 rows (35.90%) | Fold 3: 10 rows (12.82%) | Fold 4: 12 rows (15.38%) | Fold 5: 8 rows (10.26%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 2/2 [00:23<00:00, 11.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: maxDepth: 3, maxBins: 20\n",
      "Best areaUnderROC: 0.8971\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 99\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.9545\n",
      "Final Accuracy: 0.9545, Precision: 0.9167, Recall: 1.0000\n",
      "\n",
      "Sorted Feature Importances:\n",
      "Feature: diagnostics_Mask-original_VoxelNum, Importance: 0.7719\n",
      "Feature: original_firstorder_10Percentile, Importance: 0.1831\n",
      "Feature: diagnostics_Image-original_Mean, Importance: 0.0450\n",
      "Feature: diagnostics_Image-original_Dimensionality, Importance: 0.0000\n",
      "Feature: diagnostics_Image-original_Minimum, Importance: 0.0000\n",
      "Feature: diagnostics_Image-original_Maximum, Importance: 0.0000\n",
      "Feature: diagnostics_Mask-original_VolumeNum, Importance: 0.0000\n",
      "Feature: original_firstorder_90Percentile, Importance: 0.0000\n",
      "Feature: original_firstorder_Energy, Importance: 0.0000\n",
      "Feature: original_firstorder_Entropy, Importance: 0.0000\n",
      "Feature: original_firstorder_InterquartileRange, Importance: 0.0000\n",
      "Feature: original_firstorder_Kurtosis, Importance: 0.0000\n",
      "Feature: original_firstorder_Maximum, Importance: 0.0000\n",
      "Feature: original_firstorder_MeanAbsoluteDeviation, Importance: 0.0000\n",
      "Feature: original_firstorder_Mean, Importance: 0.0000\n",
      "Feature: original_firstorder_Median, Importance: 0.0000\n",
      "Feature: original_firstorder_Minimum, Importance: 0.0000\n",
      "Feature: original_firstorder_Range, Importance: 0.0000\n",
      "Feature: original_firstorder_RobustMeanAbsoluteDeviation, Importance: 0.0000\n",
      "Feature: original_firstorder_RootMeanSquared, Importance: 0.0000\n",
      "Feature: original_firstorder_Skewness, Importance: 0.0000\n",
      "Feature: original_firstorder_TotalEnergy, Importance: 0.0000\n",
      "Feature: original_firstorder_Uniformity, Importance: 0.0000\n",
      "Feature: original_firstorder_Variance, Importance: 0.0000\n",
      "Feature: original_glcm_Autocorrelation, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterProminence, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterShade, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterTendency, Importance: 0.0000\n",
      "Feature: original_glcm_Contrast, Importance: 0.0000\n",
      "Feature: original_glcm_Correlation, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceAverage, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceVariance, Importance: 0.0000\n",
      "Feature: original_glcm_Id, Importance: 0.0000\n",
      "Feature: original_glcm_Idm, Importance: 0.0000\n",
      "Feature: original_glcm_Idmn, Importance: 0.0000\n",
      "Feature: original_glcm_Idn, Importance: 0.0000\n",
      "Feature: original_glcm_Imc1, Importance: 0.0000\n",
      "Feature: original_glcm_Imc2, Importance: 0.0000\n",
      "Feature: original_glcm_InverseVariance, Importance: 0.0000\n",
      "Feature: original_glcm_JointAverage, Importance: 0.0000\n",
      "Feature: original_glcm_JointEnergy, Importance: 0.0000\n",
      "Feature: original_glcm_JointEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_MCC, Importance: 0.0000\n",
      "Feature: original_glcm_MaximumProbability, Importance: 0.0000\n",
      "Feature: original_glcm_SumAverage, Importance: 0.0000\n",
      "Feature: original_glcm_SumEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_SumSquares, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceEntropy, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceNonUniformity, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceVariance, Importance: 0.0000\n",
      "Feature: original_gldm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_gldm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_gldm_HighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_glrlm_HighGrayLevelRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LowGrayLevelRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_RunEntropy, Importance: 0.0000\n",
      "Feature: original_glrlm_RunLengthNonUniformity, Importance: 0.0000\n",
      "Feature: original_glrlm_RunLengthNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glrlm_RunPercentage, Importance: 0.0000\n",
      "Feature: original_glrlm_RunVariance, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_glszm_HighGrayLevelZoneEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LowGrayLevelZoneEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SizeZoneNonUniformity, Importance: 0.0000\n",
      "Feature: original_glszm_SizeZoneNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_ZoneEntropy, Importance: 0.0000\n",
      "Feature: original_glszm_ZonePercentage, Importance: 0.0000\n",
      "Feature: original_glszm_ZoneVariance, Importance: 0.0000\n",
      "Feature: original_ngtdm_Busyness, Importance: 0.0000\n",
      "Feature: original_ngtdm_Coarseness, Importance: 0.0000\n",
      "Feature: original_ngtdm_Complexity, Importance: 0.0000\n",
      "Feature: original_ngtdm_Contrast, Importance: 0.0000\n",
      "Feature: original_ngtdm_Strength, Importance: 0.0000\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(labelCol = \"class\", featuresCol = \"features\")\n",
    "\n",
    "# Set up parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(decision_tree.maxDepth, [3,]) \\\n",
    "    .addGrid(decision_tree.maxBins, [20, 32]) \\\n",
    "    .build()\n",
    "\n",
    "main(decision_tree, \"features_128_lesion_mask.csv\", param_grid, is_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
