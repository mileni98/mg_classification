{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import findspark\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType, FloatType, IntegerType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, PCA, StandardScaler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, LinearSVC\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "\n",
    "# Load environment variables from .env file\n",
    "_ = load_dotenv()\n",
    "\n",
    "# Retrieve environment variables\n",
    "TARGET_DIR_NAME = os.getenv(\"TARGET_DIR_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session running on 12 cores. UI is available at: http://172.20.0.1:4050\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "\n",
    "# Initialize Spark session to run locally\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Big Data Classification\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.ui.port\", \"4050\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Print the number of cores being used by Spark\n",
    "print(f\"Spark Session running on {spark.sparkContext.defaultParallelism} cores. UI is available at: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions for infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_column_types(df):\n",
    "    \"\"\"Assign StringType, IntegerType, and set default as DoubleType for other columns.\"\"\"\n",
    "\n",
    "    # Define specific StringType and IntegerType columns\n",
    "    string_columns = [\"name\", \"provider\"]\n",
    "    integer_columns = [\"patient\", \"class\"]\n",
    "\n",
    "    # Apply column types: cast to StringType, IntegerType, and default to DoubleType for others in one line\n",
    "    df = df.select([col(c).cast(StringType()) if c in string_columns\n",
    "                    else col(c).cast(IntegerType()) if c in integer_columns\n",
    "                    else col(c).cast(DoubleType()) for c in df.columns])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data: DataFrame, lesion_mask: bool) -> DataFrame:\n",
    "    \"\"\"Preprocess the data by removing rows with NaN values.\"\"\"    \n",
    "\n",
    "    # Remove rows with NaN values and count the number of rows before and after\n",
    "    total_rows_before = data.count()\n",
    "    clean_data = data.dropna() \n",
    "    \n",
    "    if lesion_mask:\n",
    "        clean_data = clean_data.filter(col('name') != 'hcs_003-001341_003-001341_MG_BL_Series-1005_Image-1005-0.png')\n",
    "    \n",
    "    # Apply column types to the cleaned data\n",
    "    clean_data = apply_column_types(clean_data)\n",
    "    total_rows_after = clean_data.count()\n",
    "\n",
    "    if total_rows_before != total_rows_after:\n",
    "        print(f\"Number of rows with NaN values: {total_rows_before - total_rows_after}\")\n",
    "\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_patient_overlap(train_df: DataFrame, test_df: DataFrame, patient_column: str = 'patient', verbose_just_error: bool = False) -> None:\n",
    "    \"\"\"Checks for overlapping patients between train and test DataFrames.\"\"\"\n",
    "\n",
    "    # Get unique patients from train and test DataFrames\n",
    "    train_patients = train_df.select(patient_column).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    test_patients = test_df.select(patient_column).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    # Convert lists to sets and check for any overlap\n",
    "    overlapping_patients = set(train_patients).intersection(set(test_patients))\n",
    "\n",
    "    # Print overlapping patients, if any\n",
    "    if overlapping_patients:\n",
    "        print(f\"Overlapping patients found: {overlapping_patients}\")\n",
    "    elif not verbose_just_error:\n",
    "        print(\"No overlapping patients between training and test sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_distribution(df1: DataFrame, df2: DataFrame, class_column: str = 'class') -> None:\n",
    "    \"\"\"Calculates and prints class distribution percentages for two DataFrames in a single line.\"\"\"\n",
    "    \n",
    "    # Calculate class counts for both DataFrames\n",
    "    df1_counts = df1.groupBy(class_column).count().orderBy(class_column).collect()\n",
    "    df2_counts = df2.groupBy(class_column).count().orderBy(class_column).collect()\n",
    "    \n",
    "    # Calculate total counts\n",
    "    total_count_df1 = df1.count()\n",
    "    total_count_df2 = df2.count()\n",
    "    \n",
    "    # Prepare class distributions as strings\n",
    "    df1_distribution = ', '.join([f\"{row[class_column]}: {row['count']} ({(row['count'] / total_count_df1) * 100:.2f}%)\" for row in df1_counts])\n",
    "    df2_distribution = ', '.join([f\"{row[class_column]}: {row['count']} ({(row['count'] / total_count_df2) * 100:.2f}%)\" for row in df2_counts])\n",
    "    \n",
    "    # Print class distributions in one line\n",
    "    print(f\"Class distribution in train_df: [{df1_distribution}] | test_df: [{df2_distribution}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_by_patient(data: DataFrame, test_ratio: float = 0.2, patient_column: str = 'patient', seed: int = 42) -> tuple[DataFrame, DataFrame]:\n",
    "    \"\"\" Splits the DataFrame into train and test sets based on unique patient IDs, ensuring no patient data overlap.\"\"\"\n",
    "\n",
    "    random.seed(seed) # Set seed for reproducibility\n",
    "    \n",
    "    # Extract unique patient IDs and randomly shuffle them\n",
    "    unique_patients = data.select(patient_column).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    random.shuffle(unique_patients)\n",
    "    \n",
    "    # Split the patients into train and test sets\n",
    "    split_index = int(len(unique_patients) * (1 - test_ratio))\n",
    "    train_patients = unique_patients[:split_index]\n",
    "    test_patients = unique_patients[split_index:]\n",
    "    \n",
    "    # Create train and test DataFrames \n",
    "    train_df = data.filter(col(patient_column).isin(train_patients))\n",
    "    test_df = data.filter(col(patient_column).isin(test_patients))\n",
    "    check_patient_overlap(train_df, test_df)\n",
    "    \n",
    "    # Calculate and print the percentages of train and test sets\n",
    "    total_rows, train_rows, test_rows = data.count(), train_df.count(), test_df.count()\n",
    "    print(f\"Training size: {train_rows} ({(train_rows / total_rows) * 100:.2f}%), Test size: {test_rows} rows ({(test_rows / total_rows) * 100:.2f}%)\") \n",
    "    \n",
    "    calculate_class_distribution(train_df, test_df)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds_by_patient(data: DataFrame, num_folds: int = 5, patient_column: str = 'patient', seed: int = 42) -> list:\n",
    "    \"\"\"Splits the data into train-test folds based on unique patient IDs, ensuring no patient overlap across folds.\"\"\"\n",
    "\n",
    "    random.seed(seed) # Set seed for reproducibility\n",
    "\n",
    "    # Extract unique patient IDs and randomly shuffle them\n",
    "    unique_patients = data.select(patient_column).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    random.shuffle(unique_patients)\n",
    "\n",
    "    # Split patients evenly into folds\n",
    "    fold_size = len(unique_patients) // num_folds\n",
    "    folds = [unique_patients[i * fold_size:(i + 1) * fold_size] for i in range(num_folds)]\n",
    "\n",
    "    # Handle any remaining patients by adding them to the last fold\n",
    "    for i in range(len(unique_patients) % num_folds):\n",
    "        folds[i].append(unique_patients[-(i + 1)])\n",
    "\n",
    "    patient_folds, fold_summaries = [], []\n",
    "\n",
    "    # Create train-test splits for each fold\n",
    "    total_rows = data.count()\n",
    "    for i in range(num_folds):\n",
    "\n",
    "        # Get the test patients for the current fold and assign the rest to the training set\n",
    "        test_patients = set(folds[i])\n",
    "        train_patients = set(unique_patients) - test_patients\n",
    "\n",
    "        # Filter the data based on the train and test patients\n",
    "        train_df = data.filter(col(patient_column).isin(train_patients))\n",
    "        test_df = data.filter(col(patient_column).isin(test_patients))\n",
    "        check_patient_overlap(train_df, test_df, verbose_just_error=True)\n",
    "\n",
    "        # Append the train-test split to the list of folds\n",
    "        patient_folds.append((train_df, test_df))\n",
    "\n",
    "        # Accumulate test set summary for this fold\n",
    "        fold_summaries.append(f\"Fold {i + 1}: {test_df.count()} rows ({(test_df.count() / total_rows) * 100:.2f}%)\")\n",
    "\n",
    "    print(\" | \".join(fold_summaries))\n",
    "\n",
    "    # Return the list of train-test tuples for each fold\n",
    "    return patient_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(data: DataFrame, output_column: str = 'pca_features', k: int = 10) -> DataFrame:\n",
    "    \"\"\"Applies PCA to reduce the dimensionality of the feature set to k principal components.\"\"\"\n",
    "    \n",
    "    # Initialize PCA with the specified number of components\n",
    "    pca = PCA(k=k, inputCol='features', outputCol=output_column)\n",
    "    \n",
    "    # Fit PCA on the data and transform the features\n",
    "    pca_model = pca.fit(data)\n",
    "    transformed_data = pca_model.transform(data)\n",
    "    \n",
    "    print(f\"PCA explained variance ratio: {pca_model.explainedVariance.toArray()}\")\n",
    "    \n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_features(data: DataFrame, class_column: str = 'class', patient_column: str = 'patient', output_column: str = 'features', verbose: bool = True, standard_scaler: bool = False) -> DataFrame:\n",
    "    \"\"\"Assembles feature columns into a single feature vector column, excluding patient and class columns.\"\"\"\n",
    "    \n",
    "    # Get the numeric columns from the DataFrame\n",
    "    numeric_columns = [field.name for field in data.schema.fields if isinstance(field.dataType, (DoubleType, FloatType, IntegerType))]\n",
    "\n",
    "    # Exclude class and patient columns\n",
    "    feature_columns = [\n",
    "        col for col in numeric_columns \n",
    "        if not col.startswith(\"diagnostics_\") and col not in [class_column, patient_column]\n",
    "    ]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Number of initial columns: {len(data.columns)}, number of feature columns: {len(feature_columns)}\")\n",
    "    \n",
    "    # Assemble features into a feature vector\n",
    "    assembler = VectorAssembler(inputCols = feature_columns, outputCol = 'assembled_features')\n",
    "    assembled_data = assembler.transform(data)\n",
    "\n",
    "    # Standardize features\n",
    "    if standard_scaler:\n",
    "        scaler = StandardScaler(inputCol=\"assembled_features\", outputCol = output_column, withStd = True, withMean = True)\n",
    "        scaler_model = scaler.fit(assembled_data)\n",
    "        scaled_data = scaler_model.transform(assembled_data)\n",
    "    else:\n",
    "        # Rename directly if no standardization is applied\n",
    "        assembled_data = assembled_data.withColumnRenamed('assembled_features', 'features')\n",
    "        scaled_data = assembled_data\n",
    "\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions: DataFrame) -> tuple:\n",
    "    \"\"\"Calculates accuracy, precision, and recall from the predictions DataFrame.\"\"\"\n",
    "\n",
    "    # Calculate true positives, true negatives, false positives, and false negatives\n",
    "    tp = predictions.filter((col('prediction') == 1) & (col('class') == 1)).count()\n",
    "    tn = predictions.filter((col('prediction') == 0) & (col('class') == 0)).count()\n",
    "    fp = predictions.filter((col('prediction') == 1) & (col('class') == 0)).count()\n",
    "    fn = predictions.filter((col('prediction') == 0) & (col('class') == 1)).count()\n",
    "\n",
    "    # Calculate accuracy, precision, and recall\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_best_parameters_on_folds(model, train_df: DataFrame, paramGrid: list[dict], num_folds: int = 5, patient_column: str = 'patient', standard_scaler: bool = False):\n",
    "    \"\"\"Evaluates the Decision Tree model using patient-wise cross-validation with specified hyperparameter tuning.\"\"\"\n",
    "\n",
    "    folds = create_folds_by_patient(train_df, num_folds, patient_column)\n",
    "\n",
    "    # Use BinaryClassificationEvaluator to measure performance\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol = \"class\", rawPredictionCol = \"prediction\", metricName = \"areaUnderROC\")\n",
    "\n",
    "    best_params, best_metric = None, float(\"-inf\")\n",
    "\n",
    "    # Iterate over each parameter combination in the grid\n",
    "    for params in tqdm(paramGrid, desc=\"Hyperparameter Tuning\", leave = True):\n",
    "        fold_metrics = []\n",
    "\n",
    "        # Print parameter combination being evaluated\n",
    "        param_items = [f\"{param.name}: {value}\" for param, value in params.items()]\n",
    "\n",
    "        # For each fold, evaluate the parameter combination\n",
    "        for _, (train_df, test_df) in enumerate(folds):\n",
    "\n",
    "            # Vectorize features\n",
    "            train_df = vectorize_features(train_df, verbose = False, standard_scaler = standard_scaler)\n",
    "            test_df = vectorize_features(test_df, verbose = False, standard_scaler = standard_scaler)\n",
    "            check_patient_overlap(train_df, test_df, verbose_just_error = True)\n",
    "\n",
    "            # Train the model with the current parameters\n",
    "            current_model = model.copy(params).fit(train_df)\n",
    "\n",
    "            # Evaluate the model on the test data using the specified metric\n",
    "            predictions = current_model.transform(test_df)\n",
    "            metric = evaluator.evaluate(predictions)\n",
    "            fold_metrics.append(metric)\n",
    "        \n",
    "        # Calculate average metric across all folds for the current parameter combination\n",
    "        avg_metric = __builtins__.sum(fold_metrics) / len(fold_metrics)\n",
    "\n",
    "        #print(f\"Average {evaluator.getMetricName()}: {avg_metric:.4f} | Evaluating Parameters:\", ', '.join(param_items))\n",
    "\n",
    "        # Update the best parameters if the current average metric is better\n",
    "        if avg_metric > best_metric:\n",
    "            best_metric = avg_metric\n",
    "            best_params = params\n",
    "\n",
    "    # Print the best parameters after evaluating all combinations\n",
    "    param_items = [f\"{param.name}: {value}\" for param, value in best_params.items()]\n",
    "    print(\"Best Overall Parameters:\", ', '.join(param_items))\n",
    "    print(f\"Best {evaluator.getMetricName()}: {best_metric:.4f}\")\n",
    "\n",
    "    return best_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_best_model(model, best_params: list[dict], train_df: DataFrame, test_df: DataFrame, is_tree: bool = False, standard_scaler: bool = False):\n",
    "    \"\"\"Train the model on the full training data using the best parameters and evaluate on the test data.\"\"\"\n",
    "\n",
    "    # Vectorize features\n",
    "    train_df = vectorize_features(train_df, verbose = False, standard_scaler = standard_scaler)\n",
    "    test_df = vectorize_features(test_df, standard_scaler=standard_scaler)\n",
    "\n",
    "    # Train the model using the best parameters\n",
    "    best_model = model.copy(best_params).fit(train_df)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"class\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "    predictions = best_model.transform(test_df)\n",
    "    metric = evaluator.evaluate(predictions)\n",
    "\n",
    "    # Calculate additional metrics: accuracy, precision, recall\n",
    "    accuracy, precision, recall = calculate_metrics(predictions)\n",
    "\n",
    "    print(f\"Final Model Evaluation on Test Data {evaluator.getMetricName()}: {metric:.4f}\")\n",
    "    print(f\"Final Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "\n",
    "    # Print feature importances if the model is a tree-based model\n",
    "    if is_tree:\n",
    "        # Extract the feature importances from the model\n",
    "        feature_importances = best_model.featureImportances.toArray()\n",
    "\n",
    "        # Get feature column names from the assembler\n",
    "        feature_columns = train_df.schema['features'].metadata['ml_attr']['attrs']['numeric']\n",
    "        feature_names = [attr['name'] for attr in feature_columns]\n",
    "\n",
    "        # Combine feature names and their importances, then sort by importance\n",
    "        sorted_features = sorted(zip(feature_names, feature_importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Print sorted feature importances\n",
    "        print(\"\\nSorted Feature Importances:\")\n",
    "        for name, importance in sorted_features:\n",
    "            print(f\"Feature: {name}, Importance: {importance:.4f}\")\n",
    "\n",
    "    # Show the first 5 names of wrongly classified rows\n",
    "    misclassified = predictions.filter(col('prediction') != col('class'))\n",
    "    names = misclassified.select('name').limit(5).rdd.flatMap(lambda x: x).collect()\n",
    "    print(\"\\nFirst 5 names of wrongly classified rows:\")\n",
    "    print(names)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model: str, file_name: str, paramGrid: list[dict], is_tree: bool = False, use_standard_scaler: bool = False):\n",
    "\n",
    "    # Load and preprocess the dataset\n",
    "    file_path = os.path.join(os.getcwd(), TARGET_DIR_NAME, file_name)\n",
    "    data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "    print(f\"\\n{'=' * 75}\\n Setting up\\n{'=' * 75}\\n\")\n",
    "    if file_name.endswith('lesion_mask.csv'):\n",
    "        clean_data = preprocess_data(data, lesion_mask = True)\n",
    "    else:\n",
    "        clean_data = preprocess_data(data, lesion_mask = False)\n",
    "    #clean_data = clean_data.limit(100)\n",
    "\n",
    "    # split the data into train and test sets\n",
    "    train_df, test_df = train_test_split_by_patient(clean_data, test_ratio=0.2)\n",
    "\n",
    "    # Evaluate the model using patient-wise cross-validation to find the best parameters\n",
    "    print(f\"\\n{'=' * 75}\\n Hyperparameter tunning\\n{'=' * 75}\\n\")\n",
    "    best_params = determine_best_parameters_on_folds(model, train_df, paramGrid, num_folds=5, patient_column='patient', standard_scaler=use_standard_scaler)\n",
    "\n",
    "    # Train the best model on the full training data and evaluate on the test data\n",
    "    print(f\"\\n{'=' * 75}\\n Testing model on test dataset\\n{'=' * 75}\\n\")\n",
    "    best_model = train_and_evaluate_best_model(model, best_params, train_df, test_df, is_tree, standard_scaler=use_standard_scaler)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 512 - with lesion mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'features_512_lesion_mask.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "Number of rows with NaN values: 1\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2173 (79.51%), Test size: 560 rows (20.49%)\n",
      "Class distribution in train_df: [0: 1086 (49.98%), 1: 1087 (50.02%)] | test_df: [0: 280 (50.00%), 1: 280 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 420 rows (19.33%) | Fold 2: 467 rows (21.49%) | Fold 3: 470 rows (21.63%) | Fold 4: 398 rows (18.32%) | Fold 5: 418 rows (19.24%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 8/8 [02:37<00:00, 19.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: maxDepth: 5, maxBins: 32\n",
      "Best areaUnderROC: 0.9790\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.9536\n",
      "Final Accuracy: 0.9536, Precision: 0.9291, Recall: 0.9821\n",
      "\n",
      "Sorted Feature Importances:\n",
      "Feature: original_gldm_GrayLevelNonUniformity, Importance: 0.9163\n",
      "Feature: original_gldm_DependenceNonUniformity, Importance: 0.0391\n",
      "Feature: original_glrlm_RunLengthNonUniformity, Importance: 0.0352\n",
      "Feature: original_firstorder_90Percentile, Importance: 0.0036\n",
      "Feature: original_glszm_SizeZoneNonUniformity, Importance: 0.0022\n",
      "Feature: original_glszm_HighGrayLevelZoneEmphasis, Importance: 0.0019\n",
      "Feature: original_firstorder_Entropy, Importance: 0.0017\n",
      "Feature: original_firstorder_10Percentile, Importance: 0.0000\n",
      "Feature: original_firstorder_Energy, Importance: 0.0000\n",
      "Feature: original_firstorder_InterquartileRange, Importance: 0.0000\n",
      "Feature: original_firstorder_Kurtosis, Importance: 0.0000\n",
      "Feature: original_firstorder_Maximum, Importance: 0.0000\n",
      "Feature: original_firstorder_MeanAbsoluteDeviation, Importance: 0.0000\n",
      "Feature: original_firstorder_Mean, Importance: 0.0000\n",
      "Feature: original_firstorder_Median, Importance: 0.0000\n",
      "Feature: original_firstorder_Minimum, Importance: 0.0000\n",
      "Feature: original_firstorder_Range, Importance: 0.0000\n",
      "Feature: original_firstorder_RobustMeanAbsoluteDeviation, Importance: 0.0000\n",
      "Feature: original_firstorder_RootMeanSquared, Importance: 0.0000\n",
      "Feature: original_firstorder_Skewness, Importance: 0.0000\n",
      "Feature: original_firstorder_TotalEnergy, Importance: 0.0000\n",
      "Feature: original_firstorder_Uniformity, Importance: 0.0000\n",
      "Feature: original_firstorder_Variance, Importance: 0.0000\n",
      "Feature: original_glcm_Autocorrelation, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterProminence, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterShade, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterTendency, Importance: 0.0000\n",
      "Feature: original_glcm_Contrast, Importance: 0.0000\n",
      "Feature: original_glcm_Correlation, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceAverage, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceVariance, Importance: 0.0000\n",
      "Feature: original_glcm_Id, Importance: 0.0000\n",
      "Feature: original_glcm_Idm, Importance: 0.0000\n",
      "Feature: original_glcm_Idmn, Importance: 0.0000\n",
      "Feature: original_glcm_Idn, Importance: 0.0000\n",
      "Feature: original_glcm_Imc1, Importance: 0.0000\n",
      "Feature: original_glcm_Imc2, Importance: 0.0000\n",
      "Feature: original_glcm_InverseVariance, Importance: 0.0000\n",
      "Feature: original_glcm_JointAverage, Importance: 0.0000\n",
      "Feature: original_glcm_JointEnergy, Importance: 0.0000\n",
      "Feature: original_glcm_JointEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_MCC, Importance: 0.0000\n",
      "Feature: original_glcm_MaximumProbability, Importance: 0.0000\n",
      "Feature: original_glcm_SumAverage, Importance: 0.0000\n",
      "Feature: original_glcm_SumEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_SumSquares, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceEntropy, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceVariance, Importance: 0.0000\n",
      "Feature: original_gldm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_gldm_HighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_glrlm_HighGrayLevelRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LowGrayLevelRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_RunEntropy, Importance: 0.0000\n",
      "Feature: original_glrlm_RunLengthNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glrlm_RunPercentage, Importance: 0.0000\n",
      "Feature: original_glrlm_RunVariance, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LowGrayLevelZoneEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SizeZoneNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_ZoneEntropy, Importance: 0.0000\n",
      "Feature: original_glszm_ZonePercentage, Importance: 0.0000\n",
      "Feature: original_glszm_ZoneVariance, Importance: 0.0000\n",
      "Feature: original_ngtdm_Busyness, Importance: 0.0000\n",
      "Feature: original_ngtdm_Coarseness, Importance: 0.0000\n",
      "Feature: original_ngtdm_Complexity, Importance: 0.0000\n",
      "Feature: original_ngtdm_Contrast, Importance: 0.0000\n",
      "Feature: original_ngtdm_Strength, Importance: 0.0000\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000070_001-000070_MG_BL_Series-3_Image-1-1.png', 'hcs_003-000295_003-000295_MG_BL_Series-1008_Image-1-1.png', 'hcs_003-001862_003-001862_MG_BL_Series-1001_Image-1001-0.png', 'hcs_003-001862_003-001862_MG_BL_Series-1001_Image-1003-0.png', 'hcs_003-001987_003-001987_MG_BL_Series-3_Image-1-0.png']\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(labelCol = \"class\", featuresCol = \"features\")\n",
    "\n",
    "# Set up parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(decision_tree.maxDepth, [5, 10, 15, 20]) \\\n",
    "    .addGrid(decision_tree.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(decision_tree, file_name, param_grid, is_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "Number of rows with NaN values: 1\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2173 (79.51%), Test size: 560 rows (20.49%)\n",
      "Class distribution in train_df: [0: 1086 (49.98%), 1: 1087 (50.02%)] | test_df: [0: 280 (50.00%), 1: 280 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 420 rows (19.33%) | Fold 2: 467 rows (21.49%) | Fold 3: 470 rows (21.63%) | Fold 4: 398 rows (18.32%) | Fold 5: 418 rows (19.24%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 72/72 [37:50<00:00, 31.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: numTrees: 150, maxDepth: 10, maxBins: 32, featureSubsetStrategy: auto\n",
      "Best areaUnderROC: 0.9879\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.9554\n",
      "Final Accuracy: 0.9554, Precision: 0.9180, Recall: 1.0000\n",
      "\n",
      "Sorted Feature Importances:\n",
      "Feature: original_gldm_DependenceNonUniformity, Importance: 0.1591\n",
      "Feature: original_gldm_GrayLevelNonUniformity, Importance: 0.1269\n",
      "Feature: original_glrlm_GrayLevelNonUniformity, Importance: 0.0960\n",
      "Feature: original_ngtdm_Busyness, Importance: 0.0706\n",
      "Feature: original_glrlm_RunLengthNonUniformity, Importance: 0.0677\n",
      "Feature: original_glszm_GrayLevelNonUniformity, Importance: 0.0529\n",
      "Feature: original_ngtdm_Coarseness, Importance: 0.0416\n",
      "Feature: original_glszm_SizeZoneNonUniformity, Importance: 0.0333\n",
      "Feature: original_ngtdm_Strength, Importance: 0.0327\n",
      "Feature: original_glrlm_LowGrayLevelRunEmphasis, Importance: 0.0233\n",
      "Feature: original_glrlm_RunLengthNonUniformityNormalized, Importance: 0.0229\n",
      "Feature: original_glrlm_ShortRunLowGrayLevelEmphasis, Importance: 0.0168\n",
      "Feature: original_glrlm_RunVariance, Importance: 0.0162\n",
      "Feature: original_firstorder_TotalEnergy, Importance: 0.0139\n",
      "Feature: original_glrlm_HighGrayLevelRunEmphasis, Importance: 0.0112\n",
      "Feature: original_firstorder_Mean, Importance: 0.0100\n",
      "Feature: original_glrlm_LongRunLowGrayLevelEmphasis, Importance: 0.0095\n",
      "Feature: original_firstorder_Range, Importance: 0.0090\n",
      "Feature: original_glcm_SumAverage, Importance: 0.0080\n",
      "Feature: original_glszm_LargeAreaLowGrayLevelEmphasis, Importance: 0.0080\n",
      "Feature: original_glcm_JointAverage, Importance: 0.0077\n",
      "Feature: original_firstorder_Energy, Importance: 0.0077\n",
      "Feature: original_glrlm_ShortRunHighGrayLevelEmphasis, Importance: 0.0074\n",
      "Feature: original_firstorder_Median, Importance: 0.0072\n",
      "Feature: original_glrlm_LongRunHighGrayLevelEmphasis, Importance: 0.0072\n",
      "Feature: original_gldm_LowGrayLevelEmphasis, Importance: 0.0069\n",
      "Feature: original_glszm_ZoneVariance, Importance: 0.0067\n",
      "Feature: original_glrlm_LongRunEmphasis, Importance: 0.0065\n",
      "Feature: original_firstorder_90Percentile, Importance: 0.0065\n",
      "Feature: original_firstorder_RootMeanSquared, Importance: 0.0061\n",
      "Feature: original_firstorder_Skewness, Importance: 0.0054\n",
      "Feature: original_glszm_HighGrayLevelZoneEmphasis, Importance: 0.0045\n",
      "Feature: original_glrlm_RunPercentage, Importance: 0.0045\n",
      "Feature: original_glszm_LargeAreaEmphasis, Importance: 0.0044\n",
      "Feature: original_gldm_LargeDependenceLowGrayLevelEmphasis, Importance: 0.0042\n",
      "Feature: original_firstorder_Minimum, Importance: 0.0042\n",
      "Feature: original_firstorder_10Percentile, Importance: 0.0040\n",
      "Feature: original_glszm_LargeAreaHighGrayLevelEmphasis, Importance: 0.0038\n",
      "Feature: original_glszm_SmallAreaHighGrayLevelEmphasis, Importance: 0.0031\n",
      "Feature: original_glszm_LowGrayLevelZoneEmphasis, Importance: 0.0031\n",
      "Feature: original_glrlm_ShortRunEmphasis, Importance: 0.0028\n",
      "Feature: original_glcm_Autocorrelation, Importance: 0.0028\n",
      "Feature: original_glrlm_RunEntropy, Importance: 0.0026\n",
      "Feature: original_firstorder_Variance, Importance: 0.0026\n",
      "Feature: original_glcm_SumEntropy, Importance: 0.0025\n",
      "Feature: original_gldm_DependenceNonUniformityNormalized, Importance: 0.0023\n",
      "Feature: original_glrlm_GrayLevelVariance, Importance: 0.0021\n",
      "Feature: original_gldm_SmallDependenceHighGrayLevelEmphasis, Importance: 0.0018\n",
      "Feature: original_firstorder_Kurtosis, Importance: 0.0018\n",
      "Feature: original_firstorder_RobustMeanAbsoluteDeviation, Importance: 0.0018\n",
      "Feature: original_glcm_SumSquares, Importance: 0.0016\n",
      "Feature: original_firstorder_InterquartileRange, Importance: 0.0015\n",
      "Feature: original_glszm_SmallAreaEmphasis, Importance: 0.0015\n",
      "Feature: original_gldm_LargeDependenceEmphasis, Importance: 0.0015\n",
      "Feature: original_glcm_Idn, Importance: 0.0014\n",
      "Feature: original_glcm_ClusterProminence, Importance: 0.0013\n",
      "Feature: original_glcm_Id, Importance: 0.0013\n",
      "Feature: original_gldm_DependenceVariance, Importance: 0.0013\n",
      "Feature: original_glszm_SmallAreaLowGrayLevelEmphasis, Importance: 0.0012\n",
      "Feature: original_glcm_Idm, Importance: 0.0012\n",
      "Feature: original_glszm_GrayLevelNonUniformityNormalized, Importance: 0.0011\n",
      "Feature: original_glcm_Contrast, Importance: 0.0011\n",
      "Feature: original_gldm_DependenceEntropy, Importance: 0.0010\n",
      "Feature: original_glcm_DifferenceEntropy, Importance: 0.0010\n",
      "Feature: original_gldm_HighGrayLevelEmphasis, Importance: 0.0010\n",
      "Feature: original_glcm_InverseVariance, Importance: 0.0010\n",
      "Feature: original_glszm_SizeZoneNonUniformityNormalized, Importance: 0.0009\n",
      "Feature: original_glcm_ClusterTendency, Importance: 0.0009\n",
      "Feature: original_firstorder_MeanAbsoluteDeviation, Importance: 0.0009\n",
      "Feature: original_glcm_Imc1, Importance: 0.0009\n",
      "Feature: original_glszm_ZonePercentage, Importance: 0.0009\n",
      "Feature: original_glcm_JointEntropy, Importance: 0.0008\n",
      "Feature: original_glcm_DifferenceVariance, Importance: 0.0007\n",
      "Feature: original_glcm_Idmn, Importance: 0.0007\n",
      "Feature: original_gldm_SmallDependenceLowGrayLevelEmphasis, Importance: 0.0007\n",
      "Feature: original_glcm_Imc2, Importance: 0.0007\n",
      "Feature: original_glrlm_GrayLevelNonUniformityNormalized, Importance: 0.0006\n",
      "Feature: original_glszm_ZoneEntropy, Importance: 0.0006\n",
      "Feature: original_firstorder_Uniformity, Importance: 0.0006\n",
      "Feature: original_glcm_ClusterShade, Importance: 0.0006\n",
      "Feature: original_glszm_GrayLevelVariance, Importance: 0.0006\n",
      "Feature: original_glcm_MCC, Importance: 0.0006\n",
      "Feature: original_ngtdm_Contrast, Importance: 0.0006\n",
      "Feature: original_ngtdm_Complexity, Importance: 0.0006\n",
      "Feature: original_gldm_SmallDependenceEmphasis, Importance: 0.0006\n",
      "Feature: original_gldm_GrayLevelVariance, Importance: 0.0005\n",
      "Feature: original_glcm_Correlation, Importance: 0.0005\n",
      "Feature: original_glcm_MaximumProbability, Importance: 0.0005\n",
      "Feature: original_glcm_DifferenceAverage, Importance: 0.0005\n",
      "Feature: original_glcm_JointEnergy, Importance: 0.0004\n",
      "Feature: original_gldm_LargeDependenceHighGrayLevelEmphasis, Importance: 0.0003\n",
      "Feature: original_firstorder_Entropy, Importance: 0.0002\n",
      "Feature: original_firstorder_Maximum, Importance: 0.0000\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['hcs_003-001862_003-001862_MG_BL_Series-1001_Image-1001-0.png', 'hcs_003-001862_003-001862_MG_BL_Series-1001_Image-1003-0.png', 'hcs_003-001987_003-001987_MG_BL_Series-1_Image-1-0.png', 'hcs_003-001987_003-001987_MG_BL_Series-3_Image-1-0.png', 'hcs_003-001987_003-001987_MG_BL_Series-4_Image-1-0.png']\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(labelCol=\"class\", featuresCol=\"features\")\n",
    "\n",
    "# Set up parameter grid for hyperparameter tuning\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(random_forest.numTrees, [50, 150, 200]) \\\n",
    "    .addGrid(random_forest.maxDepth, [5, 10, 15, 20]) \\\n",
    "    .addGrid(random_forest.maxBins, [32, 64, 128]) \\\n",
    "    .addGrid(random_forest.featureSubsetStrategy, ['auto', 'sqrt']) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(random_forest, file_name, rf_param_grid, is_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "Number of rows with NaN values: 1\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2173 (79.51%), Test size: 560 rows (20.49%)\n",
      "Class distribution in train_df: [0: 1086 (49.98%), 1: 1087 (50.02%)] | test_df: [0: 280 (50.00%), 1: 280 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 420 rows (19.33%) | Fold 2: 467 rows (21.49%) | Fold 3: 470 rows (21.63%) | Fold 4: 398 rows (18.32%) | Fold 5: 418 rows (19.24%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning:   0%|          | 0/15 [00:00<?, ?it/s]2024-09-23 18:46:11,769 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:46:20,580 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:46:27,933 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:46:32,728 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:46:38,032 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:46:42,807 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:46:48,053 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:46:52,803 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:46:58,254 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:47:03,031 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:   7%|▋         | 1/15 [00:57<13:31, 57.94s/it]2024-09-23 18:47:10,066 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:47:15,649 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:47:22,536 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:47:27,982 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:47:34,708 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:47:40,187 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:47:47,012 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:47:52,482 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:47:59,282 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:48:04,765 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  13%|█▎        | 2/15 [01:59<13:01, 60.11s/it]2024-09-23 18:48:11,542 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:48:17,159 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:48:23,848 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:48:29,537 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:48:36,307 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:48:41,909 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:48:48,755 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:48:54,402 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:49:01,205 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:49:06,865 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  20%|██        | 3/15 [03:01<12:12, 61.08s/it]2024-09-23 18:49:13,767 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:49:19,279 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:49:25,924 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:49:31,393 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:49:38,224 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:49:43,701 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:49:50,387 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:49:55,826 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:50:02,502 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:50:07,944 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  27%|██▋       | 4/15 [04:02<11:11, 61.01s/it]2024-09-23 18:50:14,642 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:50:20,273 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:50:26,998 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:50:32,624 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:50:39,335 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:50:44,883 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:50:51,620 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:50:57,199 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:51:03,953 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:51:09,553 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  33%|███▎      | 5/15 [05:04<10:12, 61.23s/it]2024-09-23 18:51:16,240 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:51:22,068 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:51:28,777 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:51:34,545 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:51:41,272 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:51:46,990 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:51:53,754 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:51:59,494 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:52:06,308 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:52:12,083 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  40%|████      | 6/15 [06:06<09:15, 61.70s/it]2024-09-23 18:52:18,856 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:52:24,364 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:52:31,228 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:52:37,134 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:52:44,546 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:52:50,324 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:52:57,333 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:53:02,996 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:53:10,007 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:53:15,717 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  47%|████▋     | 7/15 [07:10<08:19, 62.38s/it]2024-09-23 18:53:22,810 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:53:28,641 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:53:35,686 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:53:41,536 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:53:48,610 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:53:54,440 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:54:01,540 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:54:07,374 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:54:14,413 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:54:20,183 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  53%|█████▎    | 8/15 [08:15<07:21, 63.05s/it]2024-09-23 18:54:27,271 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:54:33,348 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:54:40,550 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:54:46,644 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:54:53,859 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:54:59,736 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:55:06,959 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:55:12,958 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:55:20,105 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:55:26,118 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  60%|██████    | 9/15 [09:21<06:23, 63.99s/it]2024-09-23 18:55:33,350 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:55:39,058 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:55:46,147 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:55:51,821 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:55:58,900 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:56:04,572 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:56:11,599 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:56:17,245 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:56:24,311 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:56:30,078 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  67%|██████▋   | 10/15 [10:25<05:19, 63.94s/it]2024-09-23 18:56:37,117 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:56:43,106 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:56:50,269 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:56:56,103 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:57:03,193 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:57:09,257 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:57:17,142 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:57:22,937 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:57:30,102 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:57:35,982 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  73%|███████▎  | 11/15 [11:31<04:18, 64.56s/it]2024-09-23 18:57:43,117 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:57:49,190 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:57:56,458 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:58:02,487 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:58:09,705 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:58:15,661 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:58:22,904 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:58:28,949 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:58:36,105 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:58:42,153 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  80%|████████  | 12/15 [12:37<03:15, 65.06s/it]2024-09-23 18:58:49,368 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:58:55,228 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:59:02,707 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:59:08,546 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:59:16,277 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:59:21,926 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:59:28,971 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:59:34,688 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 18:59:41,834 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 18:59:47,514 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  87%|████████▋ | 13/15 [13:42<02:10, 65.13s/it]2024-09-23 18:59:54,611 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:00:00,440 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:00:07,546 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:00:13,385 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:00:20,419 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:00:26,151 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:00:33,270 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:00:39,033 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:00:46,095 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:00:51,909 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  93%|█████████▎| 14/15 [14:46<01:04, 64.90s/it]2024-09-23 19:00:58,995 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:01:04,937 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:01:12,021 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:01:18,022 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:01:25,055 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:01:30,975 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:01:38,019 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:01:43,926 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:01:50,962 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:01:57,007 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning: 100%|██████████| 15/15 [15:52<00:00, 63.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: max_depth: 6, n_estimators: 200, learning_rate: 0.1, subsample: 0.8, colsample_bytree: 0.8\n",
      "Best areaUnderROC: 0.9896\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 19:02:01,937 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:02:07,988 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Evaluation on Test Data areaUnderROC: 0.9589\n",
      "Final Accuracy: 0.9589, Precision: 0.9241, Recall: 1.0000\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['hcs_003-001862_003-001862_MG_BL_Series-1001_Image-1001-0.png', 'hcs_003-001862_003-001862_MG_BL_Series-1001_Image-1003-0.png', 'hcs_003-001987_003-001987_MG_BL_Series-1_Image-1-0.png', 'hcs_003-001987_003-001987_MG_BL_Series-3_Image-1-0.png', 'hcs_003-001987_003-001987_MG_BL_Series-4_Image-1-0.png']\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = SparkXGBClassifier(label_col=\"class\", features_col=\"features\", use_gpu=False) \n",
    "\n",
    "# Set up the parameter grid for hyperparameter tuning\n",
    "xgb_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(xgb_classifier.max_depth, [3, 6, 9, 12, 15]) \\\n",
    "    .addGrid(xgb_classifier.n_estimators, [50, 100, 200]) \\\n",
    "    .addGrid(xgb_classifier.learning_rate, [0.1]) \\\n",
    "    .addGrid(xgb_classifier.subsample, [0.8]) \\\n",
    "    .addGrid(xgb_classifier.colsample_bytree, [0.8]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(xgb_classifier, file_name, xgb_param_grid, is_tree=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "Number of rows with NaN values: 1\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2173 (79.51%), Test size: 560 rows (20.49%)\n",
      "Class distribution in train_df: [0: 1086 (49.98%), 1: 1087 (50.02%)] | test_df: [0: 280 (50.00%), 1: 280 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 420 rows (19.33%) | Fold 2: 467 rows (21.49%) | Fold 3: 470 rows (21.63%) | Fold 4: 398 rows (18.32%) | Fold 5: 418 rows (19.24%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 12/12 [06:13<00:00, 31.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: maxIter: 100, regParam: 0.01, tol: 0.0001\n",
      "Best areaUnderROC: 0.9858\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.9554\n",
      "Final Accuracy: 0.9554, Precision: 0.9180, Recall: 1.0000\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['hcs_003-001369_003-001369_MG_BL_Series-1001_Image-1003-0.png', 'hcs_003-001862_003-001862_MG_BL_Series-1001_Image-1001-0.png', 'hcs_003-001862_003-001862_MG_BL_Series-1001_Image-1003-0.png', 'hcs_003-001987_003-001987_MG_BL_Series-1_Image-1-0.png', 'hcs_003-001987_003-001987_MG_BL_Series-3_Image-1-0.png']\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC with Standard Scaler\n",
    "svc = LinearSVC(labelCol=\"class\", featuresCol=\"features\")\n",
    "\n",
    "# Set up the parameter grid for hyperparameter tuning\n",
    "svc_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(svc.maxIter, [100, 500, 1000]) \\\n",
    "    .addGrid(svc.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(svc.tol, [1e-4,  1e-2]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function with the XGBoost classifier\n",
    "model = main(svc, file_name, svc_param_grid, is_tree=False, use_standard_scaler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 512 - with full mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'features_512_full_mask.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2162 (79.08%), Test size: 572 rows (20.92%)\n",
      "Class distribution in train_df: [0: 1081 (50.00%), 1: 1081 (50.00%)] | test_df: [0: 286 (50.00%), 1: 286 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 424 rows (19.61%) | Fold 2: 422 rows (19.52%) | Fold 3: 450 rows (20.81%) | Fold 4: 398 rows (18.41%) | Fold 5: 468 rows (21.65%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 8/8 [02:38<00:00, 19.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: maxDepth: 5, maxBins: 32\n",
      "Best areaUnderROC: 0.6755\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.6731\n",
      "Final Accuracy: 0.6731, Precision: 0.6813, Recall: 0.6503\n",
      "\n",
      "Sorted Feature Importances:\n",
      "Feature: original_firstorder_Variance, Importance: 0.5438\n",
      "Feature: original_firstorder_90Percentile, Importance: 0.0845\n",
      "Feature: original_glcm_Imc2, Importance: 0.0819\n",
      "Feature: original_firstorder_Skewness, Importance: 0.0680\n",
      "Feature: original_firstorder_10Percentile, Importance: 0.0508\n",
      "Feature: original_glrlm_RunEntropy, Importance: 0.0288\n",
      "Feature: original_glszm_HighGrayLevelZoneEmphasis, Importance: 0.0242\n",
      "Feature: original_firstorder_Kurtosis, Importance: 0.0232\n",
      "Feature: original_glcm_ClusterProminence, Importance: 0.0216\n",
      "Feature: original_glszm_SmallAreaEmphasis, Importance: 0.0172\n",
      "Feature: original_firstorder_Entropy, Importance: 0.0120\n",
      "Feature: original_glrlm_GrayLevelNonUniformityNormalized, Importance: 0.0107\n",
      "Feature: original_glcm_Imc1, Importance: 0.0105\n",
      "Feature: original_firstorder_Energy, Importance: 0.0099\n",
      "Feature: original_firstorder_InterquartileRange, Importance: 0.0070\n",
      "Feature: original_glszm_SizeZoneNonUniformityNormalized, Importance: 0.0058\n",
      "Feature: original_firstorder_Maximum, Importance: 0.0000\n",
      "Feature: original_firstorder_MeanAbsoluteDeviation, Importance: 0.0000\n",
      "Feature: original_firstorder_Mean, Importance: 0.0000\n",
      "Feature: original_firstorder_Median, Importance: 0.0000\n",
      "Feature: original_firstorder_Minimum, Importance: 0.0000\n",
      "Feature: original_firstorder_Range, Importance: 0.0000\n",
      "Feature: original_firstorder_RobustMeanAbsoluteDeviation, Importance: 0.0000\n",
      "Feature: original_firstorder_RootMeanSquared, Importance: 0.0000\n",
      "Feature: original_firstorder_TotalEnergy, Importance: 0.0000\n",
      "Feature: original_firstorder_Uniformity, Importance: 0.0000\n",
      "Feature: original_glcm_Autocorrelation, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterShade, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterTendency, Importance: 0.0000\n",
      "Feature: original_glcm_Contrast, Importance: 0.0000\n",
      "Feature: original_glcm_Correlation, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceAverage, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceVariance, Importance: 0.0000\n",
      "Feature: original_glcm_Id, Importance: 0.0000\n",
      "Feature: original_glcm_Idm, Importance: 0.0000\n",
      "Feature: original_glcm_Idmn, Importance: 0.0000\n",
      "Feature: original_glcm_Idn, Importance: 0.0000\n",
      "Feature: original_glcm_InverseVariance, Importance: 0.0000\n",
      "Feature: original_glcm_JointAverage, Importance: 0.0000\n",
      "Feature: original_glcm_JointEnergy, Importance: 0.0000\n",
      "Feature: original_glcm_JointEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_MCC, Importance: 0.0000\n",
      "Feature: original_glcm_MaximumProbability, Importance: 0.0000\n",
      "Feature: original_glcm_SumAverage, Importance: 0.0000\n",
      "Feature: original_glcm_SumEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_SumSquares, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceEntropy, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceNonUniformity, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceVariance, Importance: 0.0000\n",
      "Feature: original_gldm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_gldm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_gldm_HighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_glrlm_HighGrayLevelRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LowGrayLevelRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_RunLengthNonUniformity, Importance: 0.0000\n",
      "Feature: original_glrlm_RunLengthNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glrlm_RunPercentage, Importance: 0.0000\n",
      "Feature: original_glrlm_RunVariance, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LowGrayLevelZoneEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SizeZoneNonUniformity, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_ZoneEntropy, Importance: 0.0000\n",
      "Feature: original_glszm_ZonePercentage, Importance: 0.0000\n",
      "Feature: original_glszm_ZoneVariance, Importance: 0.0000\n",
      "Feature: original_ngtdm_Busyness, Importance: 0.0000\n",
      "Feature: original_ngtdm_Coarseness, Importance: 0.0000\n",
      "Feature: original_ngtdm_Complexity, Importance: 0.0000\n",
      "Feature: original_ngtdm_Contrast, Importance: 0.0000\n",
      "Feature: original_ngtdm_Strength, Importance: 0.0000\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000061_001-000061_MG_BL_Series-8_Image-1-0.png', 'auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1002_Image-1002-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1003_Image-1003-0.png']\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(labelCol = \"class\", featuresCol = \"features\")\n",
    "\n",
    "# Set up parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(decision_tree.maxDepth, [5, 10, 15, 20]) \\\n",
    "    .addGrid(decision_tree.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(decision_tree, file_name, param_grid, is_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2162 (79.08%), Test size: 572 rows (20.92%)\n",
      "Class distribution in train_df: [0: 1081 (50.00%), 1: 1081 (50.00%)] | test_df: [0: 286 (50.00%), 1: 286 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 424 rows (19.61%) | Fold 2: 422 rows (19.52%) | Fold 3: 450 rows (20.81%) | Fold 4: 398 rows (18.41%) | Fold 5: 468 rows (21.65%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 36/36 [33:44<00:00, 56.23s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: numTrees: 150, maxDepth: 10, maxBins: 64, featureSubsetStrategy: auto\n",
      "Best areaUnderROC: 0.6857\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.6941\n",
      "Final Accuracy: 0.6941, Precision: 0.6881, Recall: 0.7098\n",
      "\n",
      "Sorted Feature Importances:\n",
      "Feature: original_firstorder_Variance, Importance: 0.0627\n",
      "Feature: original_firstorder_90Percentile, Importance: 0.0457\n",
      "Feature: original_firstorder_MeanAbsoluteDeviation, Importance: 0.0456\n",
      "Feature: original_firstorder_InterquartileRange, Importance: 0.0366\n",
      "Feature: original_firstorder_RobustMeanAbsoluteDeviation, Importance: 0.0354\n",
      "Feature: original_firstorder_Skewness, Importance: 0.0345\n",
      "Feature: original_firstorder_Energy, Importance: 0.0289\n",
      "Feature: original_firstorder_TotalEnergy, Importance: 0.0273\n",
      "Feature: original_firstorder_Kurtosis, Importance: 0.0236\n",
      "Feature: original_firstorder_RootMeanSquared, Importance: 0.0225\n",
      "Feature: original_firstorder_10Percentile, Importance: 0.0224\n",
      "Feature: original_firstorder_Mean, Importance: 0.0174\n",
      "Feature: original_glszm_LowGrayLevelZoneEmphasis, Importance: 0.0151\n",
      "Feature: original_glszm_SmallAreaEmphasis, Importance: 0.0150\n",
      "Feature: original_glszm_SmallAreaHighGrayLevelEmphasis, Importance: 0.0148\n",
      "Feature: original_firstorder_Median, Importance: 0.0145\n",
      "Feature: original_glszm_SmallAreaLowGrayLevelEmphasis, Importance: 0.0145\n",
      "Feature: original_glszm_HighGrayLevelZoneEmphasis, Importance: 0.0134\n",
      "Feature: original_glszm_SizeZoneNonUniformityNormalized, Importance: 0.0133\n",
      "Feature: original_glrlm_RunEntropy, Importance: 0.0133\n",
      "Feature: original_glcm_MCC, Importance: 0.0127\n",
      "Feature: original_glszm_ZoneEntropy, Importance: 0.0126\n",
      "Feature: original_glcm_Imc1, Importance: 0.0123\n",
      "Feature: original_glcm_Correlation, Importance: 0.0113\n",
      "Feature: original_glrlm_HighGrayLevelRunEmphasis, Importance: 0.0113\n",
      "Feature: original_glszm_GrayLevelVariance, Importance: 0.0112\n",
      "Feature: original_glszm_GrayLevelNonUniformityNormalized, Importance: 0.0111\n",
      "Feature: original_glrlm_ShortRunHighGrayLevelEmphasis, Importance: 0.0110\n",
      "Feature: original_glrlm_RunLengthNonUniformityNormalized, Importance: 0.0107\n",
      "Feature: original_glrlm_LowGrayLevelRunEmphasis, Importance: 0.0104\n",
      "Feature: original_gldm_LargeDependenceLowGrayLevelEmphasis, Importance: 0.0103\n",
      "Feature: original_gldm_SmallDependenceHighGrayLevelEmphasis, Importance: 0.0102\n",
      "Feature: original_glcm_Imc2, Importance: 0.0101\n",
      "Feature: original_glrlm_ShortRunLowGrayLevelEmphasis, Importance: 0.0097\n",
      "Feature: original_glrlm_LongRunLowGrayLevelEmphasis, Importance: 0.0096\n",
      "Feature: original_glrlm_LongRunEmphasis, Importance: 0.0095\n",
      "Feature: original_glrlm_RunVariance, Importance: 0.0093\n",
      "Feature: original_glcm_ClusterShade, Importance: 0.0092\n",
      "Feature: original_glcm_ClusterProminence, Importance: 0.0090\n",
      "Feature: original_glrlm_ShortRunEmphasis, Importance: 0.0088\n",
      "Feature: original_glszm_SizeZoneNonUniformity, Importance: 0.0083\n",
      "Feature: original_glszm_GrayLevelNonUniformity, Importance: 0.0080\n",
      "Feature: original_glrlm_LongRunHighGrayLevelEmphasis, Importance: 0.0075\n",
      "Feature: original_glrlm_GrayLevelNonUniformityNormalized, Importance: 0.0075\n",
      "Feature: original_glrlm_GrayLevelVariance, Importance: 0.0074\n",
      "Feature: original_glszm_LargeAreaEmphasis, Importance: 0.0071\n",
      "Feature: original_glcm_ClusterTendency, Importance: 0.0070\n",
      "Feature: original_glszm_ZonePercentage, Importance: 0.0070\n",
      "Feature: original_glszm_LargeAreaHighGrayLevelEmphasis, Importance: 0.0068\n",
      "Feature: original_glszm_LargeAreaLowGrayLevelEmphasis, Importance: 0.0068\n",
      "Feature: original_gldm_SmallDependenceLowGrayLevelEmphasis, Importance: 0.0067\n",
      "Feature: original_gldm_SmallDependenceEmphasis, Importance: 0.0066\n",
      "Feature: original_glcm_JointEntropy, Importance: 0.0064\n",
      "Feature: original_gldm_LargeDependenceHighGrayLevelEmphasis, Importance: 0.0062\n",
      "Feature: original_glrlm_RunLengthNonUniformity, Importance: 0.0062\n",
      "Feature: original_glcm_Autocorrelation, Importance: 0.0061\n",
      "Feature: original_glcm_SumAverage, Importance: 0.0061\n",
      "Feature: original_glcm_JointEnergy, Importance: 0.0058\n",
      "Feature: original_glcm_MaximumProbability, Importance: 0.0058\n",
      "Feature: original_gldm_DependenceNonUniformity, Importance: 0.0057\n",
      "Feature: original_ngtdm_Contrast, Importance: 0.0057\n",
      "Feature: original_gldm_LowGrayLevelEmphasis, Importance: 0.0056\n",
      "Feature: original_firstorder_Entropy, Importance: 0.0055\n",
      "Feature: original_gldm_DependenceEntropy, Importance: 0.0055\n",
      "Feature: original_ngtdm_Busyness, Importance: 0.0054\n",
      "Feature: original_glszm_ZoneVariance, Importance: 0.0054\n",
      "Feature: original_gldm_DependenceNonUniformityNormalized, Importance: 0.0053\n",
      "Feature: original_firstorder_Uniformity, Importance: 0.0052\n",
      "Feature: original_glcm_SumEntropy, Importance: 0.0052\n",
      "Feature: original_glcm_JointAverage, Importance: 0.0051\n",
      "Feature: original_gldm_GrayLevelNonUniformity, Importance: 0.0051\n",
      "Feature: original_glcm_Idn, Importance: 0.0050\n",
      "Feature: original_gldm_HighGrayLevelEmphasis, Importance: 0.0050\n",
      "Feature: original_gldm_LargeDependenceEmphasis, Importance: 0.0049\n",
      "Feature: original_gldm_GrayLevelVariance, Importance: 0.0049\n",
      "Feature: original_ngtdm_Complexity, Importance: 0.0049\n",
      "Feature: original_glcm_Idm, Importance: 0.0048\n",
      "Feature: original_glrlm_GrayLevelNonUniformity, Importance: 0.0048\n",
      "Feature: original_gldm_DependenceVariance, Importance: 0.0043\n",
      "Feature: original_glcm_Idmn, Importance: 0.0043\n",
      "Feature: original_glcm_SumSquares, Importance: 0.0041\n",
      "Feature: original_glcm_DifferenceVariance, Importance: 0.0040\n",
      "Feature: original_ngtdm_Coarseness, Importance: 0.0040\n",
      "Feature: original_glcm_Id, Importance: 0.0038\n",
      "Feature: original_glrlm_RunPercentage, Importance: 0.0034\n",
      "Feature: original_glcm_DifferenceEntropy, Importance: 0.0034\n",
      "Feature: original_glcm_InverseVariance, Importance: 0.0034\n",
      "Feature: original_glcm_Contrast, Importance: 0.0033\n",
      "Feature: original_glcm_DifferenceAverage, Importance: 0.0032\n",
      "Feature: original_ngtdm_Strength, Importance: 0.0030\n",
      "Feature: original_firstorder_Range, Importance: 0.0003\n",
      "Feature: original_firstorder_Minimum, Importance: 0.0002\n",
      "Feature: original_firstorder_Maximum, Importance: 0.0001\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-1.png', 'hcs_003-000029_003-000029_MG_BL_Series-1004_Image-1004-0.png', 'hcs_003-000042_003-000042_MG_BL_Series-1001_Image-1001-1.png']\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(labelCol=\"class\", featuresCol=\"features\")\n",
    "\n",
    "# Set up parameter grid for hyperparameter tuning\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(random_forest.numTrees, [50, 150]) \\\n",
    "    .addGrid(random_forest.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(random_forest.maxBins, [32, 64, 128]) \\\n",
    "    .addGrid(random_forest.featureSubsetStrategy, ['auto', 'sqrt']) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(random_forest, file_name, rf_param_grid, is_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2162 (79.08%), Test size: 572 rows (20.92%)\n",
      "Class distribution in train_df: [0: 1081 (50.00%), 1: 1081 (50.00%)] | test_df: [0: 286 (50.00%), 1: 286 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 424 rows (19.61%) | Fold 2: 422 rows (19.52%) | Fold 3: 450 rows (20.81%) | Fold 4: 398 rows (18.41%) | Fold 5: 468 rows (21.65%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning:   0%|          | 0/15 [00:00<?, ?it/s]2024-09-23 19:47:08,243 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:47:13,099 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:47:19,461 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:47:24,962 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:47:31,702 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:47:37,132 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:47:42,326 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:47:47,146 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:47:52,440 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:47:57,278 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:   7%|▋         | 1/15 [00:55<12:54, 55.31s/it]2024-09-23 19:48:04,061 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:48:09,651 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:48:16,544 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:48:22,044 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:48:28,947 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:48:34,565 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:48:41,297 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:48:47,228 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:48:54,096 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:48:59,173 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  13%|█▎        | 2/15 [01:56<12:42, 58.66s/it]2024-09-23 19:49:04,575 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:49:10,313 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:49:17,170 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:49:22,994 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:49:29,885 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:49:35,640 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:49:42,509 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:49:48,330 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:49:55,228 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:50:00,972 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  20%|██        | 3/15 [02:59<12:06, 60.50s/it]2024-09-23 19:50:07,856 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:50:13,807 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:50:20,696 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:50:26,612 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:50:33,546 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:50:39,339 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:50:46,207 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:50:52,171 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:50:58,948 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:51:04,863 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  27%|██▋       | 4/15 [04:02<11:20, 61.83s/it]2024-09-23 19:51:11,620 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:51:17,996 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:51:24,784 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:51:31,084 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:51:37,874 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:51:44,340 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:51:51,127 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:51:57,430 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:52:04,237 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:52:10,575 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  33%|███▎      | 5/15 [05:08<10:32, 63.26s/it]2024-09-23 19:52:17,442 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:52:24,777 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:52:31,673 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:52:38,919 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:52:45,809 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:52:53,024 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:52:59,880 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:53:07,146 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:53:13,995 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:53:21,265 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  40%|████      | 6/15 [06:19<09:52, 65.82s/it]2024-09-23 19:53:28,179 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:53:34,795 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:53:41,605 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:53:48,155 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:53:55,087 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:54:01,819 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:54:08,607 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:54:15,274 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:54:22,143 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:54:28,829 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  47%|████▋     | 7/15 [07:26<08:51, 66.38s/it]2024-09-23 19:54:35,765 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:54:43,352 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:54:50,344 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:54:57,927 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:55:04,883 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:55:12,562 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:55:19,531 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:55:27,112 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:55:34,107 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:55:41,688 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  53%|█████▎    | 8/15 [08:39<07:59, 68.46s/it]2024-09-23 19:55:48,649 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:55:58,192 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:56:05,478 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:56:14,547 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:56:21,951 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:56:31,084 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:56:38,263 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:56:47,378 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:56:54,475 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:57:03,497 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  60%|██████    | 9/15 [10:01<07:15, 72.65s/it]2024-09-23 19:57:11,279 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:57:19,960 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:57:26,985 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:57:34,439 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:57:41,543 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:57:48,969 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:57:56,038 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:58:03,428 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:58:10,428 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:58:17,858 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  67%|██████▋   | 10/15 [11:16<06:05, 73.17s/it]2024-09-23 19:58:25,006 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:58:33,545 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:58:40,777 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:58:49,268 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:58:56,437 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:59:05,244 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:59:12,208 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:59:20,711 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 19:59:27,711 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:59:36,227 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  73%|███████▎  | 11/15 [12:34<04:59, 74.77s/it]2024-09-23 19:59:43,244 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 19:59:53,204 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:00:00,490 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:00:10,594 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:00:17,975 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:00:28,068 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:00:35,256 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:00:45,279 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:00:52,605 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:01:02,651 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  80%|████████  | 12/15 [14:00<03:54, 78.32s/it]2024-09-23 20:01:09,860 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:01:17,630 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:01:24,926 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:01:32,766 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:01:40,036 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:01:47,836 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:01:55,115 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:02:02,908 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:02:10,009 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:02:17,981 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  87%|████████▋ | 13/15 [15:16<02:34, 77.41s/it]2024-09-23 20:02:25,288 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:02:34,369 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:02:41,573 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:02:50,609 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:02:57,824 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:03:06,787 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:03:13,849 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:03:22,960 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:03:30,157 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:03:39,182 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  93%|█████████▎| 14/15 [16:37<01:18, 78.59s/it]2024-09-23 20:03:46,595 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:03:57,116 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:04:04,505 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:04:15,364 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:04:22,532 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:04:32,914 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:04:40,129 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:04:50,426 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:04:57,610 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:05:07,926 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning: 100%|██████████| 15/15 [18:06<00:00, 72.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: max_depth: 3, n_estimators: 100, learning_rate: 0.1, subsample: 0.8, colsample_bytree: 0.8\n",
      "Best areaUnderROC: 0.7016\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 20:05:13,279 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:05:19,417 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Evaluation on Test Data areaUnderROC: 0.7133\n",
      "Final Accuracy: 0.7133, Precision: 0.6930, Recall: 0.7657\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000061_001-000061_MG_BL_Series-8_Image-1-0.png', 'auth_001-000084_001-000084_MG_BL_Series-1_Image-1-1.png', 'auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-1.png']\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = SparkXGBClassifier(label_col=\"class\", features_col=\"features\", use_gpu=False) \n",
    "\n",
    "# Set up the parameter grid for hyperparameter tuning\n",
    "xgb_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(xgb_classifier.max_depth, [3, 6, 9, 12, 15]) \\\n",
    "    .addGrid(xgb_classifier.n_estimators, [50, 100, 200]) \\\n",
    "    .addGrid(xgb_classifier.learning_rate, [0.1]) \\\n",
    "    .addGrid(xgb_classifier.subsample, [0.8]) \\\n",
    "    .addGrid(xgb_classifier.colsample_bytree, [0.8]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(xgb_classifier, file_name, xgb_param_grid, is_tree=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2162 (79.08%), Test size: 572 rows (20.92%)\n",
      "Class distribution in train_df: [0: 1081 (50.00%), 1: 1081 (50.00%)] | test_df: [0: 286 (50.00%), 1: 286 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 424 rows (19.61%) | Fold 2: 422 rows (19.52%) | Fold 3: 450 rows (20.81%) | Fold 4: 398 rows (18.41%) | Fold 5: 468 rows (21.65%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 12/12 [07:08<00:00, 35.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: maxIter: 500, regParam: 0.01, tol: 0.0001\n",
      "Best areaUnderROC: 0.6994\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.6993\n",
      "Final Accuracy: 0.6993, Precision: 0.6887, Recall: 0.7273\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000061_001-000061_MG_BL_Series-8_Image-1-0.png', 'auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-1.png', 'hcs_003-000029_003-000029_MG_BL_Series-1002_Image-1002-0.png']\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC with Standard Scaler\n",
    "svc = LinearSVC(labelCol=\"class\", featuresCol=\"features\")\n",
    "\n",
    "# Set up the parameter grid for hyperparameter tuning\n",
    "svc_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(svc.maxIter, [100, 500, 1000]) \\\n",
    "    .addGrid(svc.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(svc.tol, [1e-4, 1e-2]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function with the XGBoost classifier\n",
    "model = main(svc, file_name, svc_param_grid, is_tree=False, use_standard_scaler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 256 - with lesion mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'features_256_lesion_mask.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "Number of rows with NaN values: 1\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2254 (80.73%), Test size: 538 rows (19.27%)\n",
      "Class distribution in train_df: [0: 1126 (49.96%), 1: 1128 (50.04%)] | test_df: [0: 269 (50.00%), 1: 269 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 510 rows (22.63%) | Fold 2: 414 rows (18.37%) | Fold 3: 417 rows (18.50%) | Fold 4: 474 rows (21.03%) | Fold 5: 439 rows (19.48%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 8/8 [02:26<00:00, 18.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: maxDepth: 5, maxBins: 32\n",
      "Best areaUnderROC: 0.9199\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.9108\n",
      "Final Accuracy: 0.9108, Precision: 0.8850, Recall: 0.9442\n",
      "\n",
      "Sorted Feature Importances:\n",
      "Feature: original_glrlm_RunLengthNonUniformity, Importance: 0.5965\n",
      "Feature: original_gldm_GrayLevelNonUniformity, Importance: 0.2463\n",
      "Feature: original_firstorder_90Percentile, Importance: 0.1001\n",
      "Feature: original_gldm_LargeDependenceLowGrayLevelEmphasis, Importance: 0.0144\n",
      "Feature: original_gldm_DependenceNonUniformity, Importance: 0.0097\n",
      "Feature: original_firstorder_Minimum, Importance: 0.0085\n",
      "Feature: original_gldm_DependenceEntropy, Importance: 0.0068\n",
      "Feature: original_ngtdm_Coarseness, Importance: 0.0063\n",
      "Feature: original_glszm_SmallAreaHighGrayLevelEmphasis, Importance: 0.0041\n",
      "Feature: original_firstorder_InterquartileRange, Importance: 0.0033\n",
      "Feature: original_glszm_GrayLevelNonUniformityNormalized, Importance: 0.0021\n",
      "Feature: original_firstorder_MeanAbsoluteDeviation, Importance: 0.0020\n",
      "Feature: original_firstorder_10Percentile, Importance: 0.0000\n",
      "Feature: original_firstorder_Energy, Importance: 0.0000\n",
      "Feature: original_firstorder_Entropy, Importance: 0.0000\n",
      "Feature: original_firstorder_Kurtosis, Importance: 0.0000\n",
      "Feature: original_firstorder_Maximum, Importance: 0.0000\n",
      "Feature: original_firstorder_Mean, Importance: 0.0000\n",
      "Feature: original_firstorder_Median, Importance: 0.0000\n",
      "Feature: original_firstorder_Range, Importance: 0.0000\n",
      "Feature: original_firstorder_RobustMeanAbsoluteDeviation, Importance: 0.0000\n",
      "Feature: original_firstorder_RootMeanSquared, Importance: 0.0000\n",
      "Feature: original_firstorder_Skewness, Importance: 0.0000\n",
      "Feature: original_firstorder_TotalEnergy, Importance: 0.0000\n",
      "Feature: original_firstorder_Uniformity, Importance: 0.0000\n",
      "Feature: original_firstorder_Variance, Importance: 0.0000\n",
      "Feature: original_glcm_Autocorrelation, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterProminence, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterShade, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterTendency, Importance: 0.0000\n",
      "Feature: original_glcm_Contrast, Importance: 0.0000\n",
      "Feature: original_glcm_Correlation, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceAverage, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceVariance, Importance: 0.0000\n",
      "Feature: original_glcm_Id, Importance: 0.0000\n",
      "Feature: original_glcm_Idm, Importance: 0.0000\n",
      "Feature: original_glcm_Idmn, Importance: 0.0000\n",
      "Feature: original_glcm_Idn, Importance: 0.0000\n",
      "Feature: original_glcm_Imc1, Importance: 0.0000\n",
      "Feature: original_glcm_Imc2, Importance: 0.0000\n",
      "Feature: original_glcm_InverseVariance, Importance: 0.0000\n",
      "Feature: original_glcm_JointAverage, Importance: 0.0000\n",
      "Feature: original_glcm_JointEnergy, Importance: 0.0000\n",
      "Feature: original_glcm_JointEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_MCC, Importance: 0.0000\n",
      "Feature: original_glcm_MaximumProbability, Importance: 0.0000\n",
      "Feature: original_glcm_SumAverage, Importance: 0.0000\n",
      "Feature: original_glcm_SumEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_SumSquares, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceVariance, Importance: 0.0000\n",
      "Feature: original_gldm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_gldm_HighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_glrlm_HighGrayLevelRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LowGrayLevelRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_RunEntropy, Importance: 0.0000\n",
      "Feature: original_glrlm_RunLengthNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glrlm_RunPercentage, Importance: 0.0000\n",
      "Feature: original_glrlm_RunVariance, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_glszm_HighGrayLevelZoneEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LowGrayLevelZoneEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SizeZoneNonUniformity, Importance: 0.0000\n",
      "Feature: original_glszm_SizeZoneNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_ZoneEntropy, Importance: 0.0000\n",
      "Feature: original_glszm_ZonePercentage, Importance: 0.0000\n",
      "Feature: original_glszm_ZoneVariance, Importance: 0.0000\n",
      "Feature: original_ngtdm_Busyness, Importance: 0.0000\n",
      "Feature: original_ngtdm_Complexity, Importance: 0.0000\n",
      "Feature: original_ngtdm_Contrast, Importance: 0.0000\n",
      "Feature: original_ngtdm_Strength, Importance: 0.0000\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000084_001-000084_MG_BL_Series-1_Image-1-0.png', 'auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000245_003-000245_MG_BL_Series-1008_Image-1-0.png', 'hcs_003-000245_003-000245_MG_BL_Series-1008_Image-1-1.png', 'hcs_003-000257_003-000257_MG_BL_Series-1010_Image-5-0.png']\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(labelCol = \"class\", featuresCol = \"features\")\n",
    "\n",
    "# Set up parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(decision_tree.maxDepth, [5, 10, 15, 20]) \\\n",
    "    .addGrid(decision_tree.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(decision_tree, file_name, param_grid, is_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "Number of rows with NaN values: 1\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2254 (80.73%), Test size: 538 rows (19.27%)\n",
      "Class distribution in train_df: [0: 1126 (49.96%), 1: 1128 (50.04%)] | test_df: [0: 269 (50.00%), 1: 269 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 510 rows (22.63%) | Fold 2: 414 rows (18.37%) | Fold 3: 417 rows (18.50%) | Fold 4: 474 rows (21.03%) | Fold 5: 439 rows (19.48%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 36/36 [21:19<00:00, 35.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: numTrees: 150, maxDepth: 15, maxBins: 64, featureSubsetStrategy: auto\n",
      "Best areaUnderROC: 0.9404\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.9461\n",
      "Final Accuracy: 0.9461, Precision: 0.9348, Recall: 0.9591\n",
      "\n",
      "Sorted Feature Importances:\n",
      "Feature: original_glrlm_RunLengthNonUniformity, Importance: 0.1019\n",
      "Feature: original_gldm_GrayLevelNonUniformity, Importance: 0.0830\n",
      "Feature: original_glrlm_GrayLevelNonUniformity, Importance: 0.0710\n",
      "Feature: original_firstorder_Median, Importance: 0.0596\n",
      "Feature: original_gldm_DependenceNonUniformity, Importance: 0.0527\n",
      "Feature: original_ngtdm_Busyness, Importance: 0.0438\n",
      "Feature: original_glszm_GrayLevelNonUniformity, Importance: 0.0429\n",
      "Feature: original_firstorder_RootMeanSquared, Importance: 0.0376\n",
      "Feature: original_firstorder_Mean, Importance: 0.0353\n",
      "Feature: original_glszm_SizeZoneNonUniformity, Importance: 0.0326\n",
      "Feature: original_firstorder_90Percentile, Importance: 0.0269\n",
      "Feature: original_firstorder_Skewness, Importance: 0.0228\n",
      "Feature: original_glrlm_RunLengthNonUniformityNormalized, Importance: 0.0227\n",
      "Feature: original_ngtdm_Coarseness, Importance: 0.0206\n",
      "Feature: original_glrlm_LowGrayLevelRunEmphasis, Importance: 0.0130\n",
      "Feature: original_ngtdm_Strength, Importance: 0.0115\n",
      "Feature: original_firstorder_10Percentile, Importance: 0.0107\n",
      "Feature: original_firstorder_RobustMeanAbsoluteDeviation, Importance: 0.0098\n",
      "Feature: original_firstorder_TotalEnergy, Importance: 0.0098\n",
      "Feature: original_glszm_HighGrayLevelZoneEmphasis, Importance: 0.0094\n",
      "Feature: original_glcm_SumAverage, Importance: 0.0091\n",
      "Feature: original_firstorder_InterquartileRange, Importance: 0.0088\n",
      "Feature: original_glrlm_RunVariance, Importance: 0.0087\n",
      "Feature: original_firstorder_Energy, Importance: 0.0082\n",
      "Feature: original_glrlm_ShortRunLowGrayLevelEmphasis, Importance: 0.0076\n",
      "Feature: original_firstorder_Range, Importance: 0.0069\n",
      "Feature: original_glrlm_RunEntropy, Importance: 0.0067\n",
      "Feature: original_glszm_SmallAreaLowGrayLevelEmphasis, Importance: 0.0063\n",
      "Feature: original_firstorder_Minimum, Importance: 0.0062\n",
      "Feature: original_firstorder_Variance, Importance: 0.0061\n",
      "Feature: original_glszm_LargeAreaEmphasis, Importance: 0.0061\n",
      "Feature: original_glcm_JointAverage, Importance: 0.0059\n",
      "Feature: original_glszm_LargeAreaLowGrayLevelEmphasis, Importance: 0.0059\n",
      "Feature: original_glszm_GrayLevelVariance, Importance: 0.0058\n",
      "Feature: original_glrlm_LongRunLowGrayLevelEmphasis, Importance: 0.0057\n",
      "Feature: original_gldm_LowGrayLevelEmphasis, Importance: 0.0056\n",
      "Feature: original_firstorder_Kurtosis, Importance: 0.0056\n",
      "Feature: original_glszm_SmallAreaEmphasis, Importance: 0.0055\n",
      "Feature: original_glcm_Autocorrelation, Importance: 0.0055\n",
      "Feature: original_firstorder_MeanAbsoluteDeviation, Importance: 0.0054\n",
      "Feature: original_glrlm_HighGrayLevelRunEmphasis, Importance: 0.0053\n",
      "Feature: original_glszm_ZonePercentage, Importance: 0.0051\n",
      "Feature: original_glszm_SmallAreaHighGrayLevelEmphasis, Importance: 0.0051\n",
      "Feature: original_gldm_SmallDependenceLowGrayLevelEmphasis, Importance: 0.0050\n",
      "Feature: original_glszm_ZoneVariance, Importance: 0.0049\n",
      "Feature: original_glcm_ClusterProminence, Importance: 0.0047\n",
      "Feature: original_glrlm_LongRunEmphasis, Importance: 0.0047\n",
      "Feature: original_glszm_LowGrayLevelZoneEmphasis, Importance: 0.0045\n",
      "Feature: original_gldm_SmallDependenceHighGrayLevelEmphasis, Importance: 0.0043\n",
      "Feature: original_glszm_LargeAreaHighGrayLevelEmphasis, Importance: 0.0039\n",
      "Feature: original_glcm_Correlation, Importance: 0.0038\n",
      "Feature: original_glcm_Imc2, Importance: 0.0037\n",
      "Feature: original_glrlm_ShortRunHighGrayLevelEmphasis, Importance: 0.0037\n",
      "Feature: original_gldm_LargeDependenceLowGrayLevelEmphasis, Importance: 0.0036\n",
      "Feature: original_glszm_SizeZoneNonUniformityNormalized, Importance: 0.0036\n",
      "Feature: original_glrlm_RunPercentage, Importance: 0.0036\n",
      "Feature: original_gldm_HighGrayLevelEmphasis, Importance: 0.0036\n",
      "Feature: original_glrlm_LongRunHighGrayLevelEmphasis, Importance: 0.0035\n",
      "Feature: original_glszm_GrayLevelNonUniformityNormalized, Importance: 0.0034\n",
      "Feature: original_glcm_Imc1, Importance: 0.0034\n",
      "Feature: original_glszm_ZoneEntropy, Importance: 0.0033\n",
      "Feature: original_glcm_Idn, Importance: 0.0030\n",
      "Feature: original_gldm_SmallDependenceEmphasis, Importance: 0.0029\n",
      "Feature: original_glrlm_ShortRunEmphasis, Importance: 0.0029\n",
      "Feature: original_glcm_MCC, Importance: 0.0028\n",
      "Feature: original_gldm_LargeDependenceHighGrayLevelEmphasis, Importance: 0.0026\n",
      "Feature: original_gldm_DependenceEntropy, Importance: 0.0025\n",
      "Feature: original_glcm_Idm, Importance: 0.0025\n",
      "Feature: original_gldm_DependenceNonUniformityNormalized, Importance: 0.0024\n",
      "Feature: original_gldm_DependenceVariance, Importance: 0.0023\n",
      "Feature: original_glcm_ClusterTendency, Importance: 0.0023\n",
      "Feature: original_firstorder_Entropy, Importance: 0.0023\n",
      "Feature: original_gldm_GrayLevelVariance, Importance: 0.0023\n",
      "Feature: original_glrlm_GrayLevelVariance, Importance: 0.0023\n",
      "Feature: original_glcm_ClusterShade, Importance: 0.0022\n",
      "Feature: original_glcm_SumSquares, Importance: 0.0022\n",
      "Feature: original_glcm_SumEntropy, Importance: 0.0022\n",
      "Feature: original_ngtdm_Contrast, Importance: 0.0021\n",
      "Feature: original_glrlm_GrayLevelNonUniformityNormalized, Importance: 0.0020\n",
      "Feature: original_glcm_JointEnergy, Importance: 0.0020\n",
      "Feature: original_glcm_JointEntropy, Importance: 0.0019\n",
      "Feature: original_glcm_MaximumProbability, Importance: 0.0018\n",
      "Feature: original_firstorder_Uniformity, Importance: 0.0018\n",
      "Feature: original_ngtdm_Complexity, Importance: 0.0016\n",
      "Feature: original_gldm_LargeDependenceEmphasis, Importance: 0.0016\n",
      "Feature: original_glcm_DifferenceEntropy, Importance: 0.0015\n",
      "Feature: original_glcm_DifferenceVariance, Importance: 0.0014\n",
      "Feature: original_glcm_Idmn, Importance: 0.0014\n",
      "Feature: original_glcm_InverseVariance, Importance: 0.0013\n",
      "Feature: original_glcm_DifferenceAverage, Importance: 0.0012\n",
      "Feature: original_glcm_Id, Importance: 0.0011\n",
      "Feature: original_glcm_Contrast, Importance: 0.0009\n",
      "Feature: original_firstorder_Maximum, Importance: 0.0006\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000084_001-000084_MG_BL_Series-1_Image-1-0.png', 'auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000017_003-000017_MG_BL_Series-1004_Image-1004-1.png', 'hcs_003-000245_003-000245_MG_BL_Series-1008_Image-1-0.png', 'hcs_003-000259_003-000259_MG_BL_Series-2_Image-1-1.png']\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(labelCol=\"class\", featuresCol=\"features\")\n",
    "\n",
    "# Set up parameter grid for hyperparameter tuning\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(random_forest.numTrees, [50, 150]) \\\n",
    "    .addGrid(random_forest.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(random_forest.maxBins, [32, 64, 128]) \\\n",
    "    .addGrid(random_forest.featureSubsetStrategy, ['auto', 'sqrt']) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(random_forest, file_name, rf_param_grid, is_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "Number of rows with NaN values: 1\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2254 (80.73%), Test size: 538 rows (19.27%)\n",
      "Class distribution in train_df: [0: 1126 (49.96%), 1: 1128 (50.04%)] | test_df: [0: 269 (50.00%), 1: 269 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 510 rows (22.63%) | Fold 2: 414 rows (18.37%) | Fold 3: 417 rows (18.50%) | Fold 4: 474 rows (21.03%) | Fold 5: 439 rows (19.48%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning:   0%|          | 0/15 [00:00<?, ?it/s]2024-09-23 20:38:40,627 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:38:45,305 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:38:50,598 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:38:55,327 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:39:00,564 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:39:05,396 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:39:10,761 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:39:15,514 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:39:22,409 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:39:28,083 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:   7%|▋         | 1/15 [00:54<12:38, 54.20s/it]2024-09-23 20:39:36,032 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:39:42,245 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:39:49,233 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:39:55,073 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:40:02,038 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:40:07,951 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:40:14,969 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:40:20,604 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:40:27,939 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:40:33,620 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  13%|█▎        | 2/15 [01:59<13:09, 60.75s/it]2024-09-23 20:40:40,831 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:40:46,849 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:40:54,114 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:40:59,952 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:41:07,216 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:41:13,235 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:41:20,501 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:41:26,376 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:41:33,669 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:41:39,568 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  20%|██        | 3/15 [03:05<12:36, 63.03s/it]2024-09-23 20:41:46,806 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:41:52,610 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:41:59,763 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:42:05,578 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:42:12,719 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:42:18,570 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:42:25,604 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:42:31,566 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:42:38,599 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:42:44,619 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  27%|██▋       | 4/15 [04:10<11:41, 63.82s/it]2024-09-23 20:42:51,627 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:42:57,915 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:43:05,114 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:43:11,433 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:43:18,470 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:43:24,802 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:43:31,928 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:43:38,254 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:43:45,600 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:43:52,029 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  33%|███▎      | 5/15 [05:17<10:51, 65.15s/it]2024-09-23 20:43:59,348 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:44:06,185 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:44:13,488 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:44:20,350 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:44:27,639 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:44:34,507 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:44:41,575 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:44:48,316 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:44:55,369 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:45:02,299 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  40%|████      | 6/15 [06:28<10:02, 66.90s/it]2024-09-23 20:45:09,615 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:45:15,846 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:45:22,917 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:45:29,178 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:45:36,202 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:45:42,463 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:45:49,472 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:45:55,679 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:46:02,804 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:46:09,048 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  47%|████▋     | 7/15 [07:34<08:54, 66.84s/it]2024-09-23 20:46:16,188 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:46:22,754 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:46:29,838 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:46:36,531 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:46:43,562 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:46:50,200 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:46:57,265 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:47:03,783 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:47:10,881 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:47:17,715 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  53%|█████▎    | 8/15 [08:44<07:54, 67.84s/it]2024-09-23 20:47:26,348 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:47:33,332 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:47:40,239 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:47:47,253 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:47:54,244 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:48:01,211 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:48:08,157 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:48:14,960 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:48:21,870 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:48:28,824 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  60%|██████    | 9/15 [09:54<06:50, 68.41s/it]2024-09-23 20:48:35,689 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:48:41,886 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:48:48,769 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:48:55,047 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:49:01,934 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:49:08,395 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:49:15,252 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:49:21,407 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:49:28,249 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:49:34,595 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  67%|██████▋   | 10/15 [11:00<05:37, 67.58s/it]2024-09-23 20:49:41,483 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:49:48,141 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:49:55,069 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:50:01,744 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:50:08,570 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:50:15,291 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:50:22,211 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:50:28,720 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:50:35,587 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:50:42,248 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  73%|███████▎  | 11/15 [12:07<04:30, 67.61s/it]2024-09-23 20:50:49,178 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:50:56,132 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:51:03,110 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:51:10,202 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:51:17,100 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:51:24,251 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:51:31,550 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:51:38,478 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:51:45,433 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:51:52,566 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  80%|████████  | 12/15 [13:18<03:25, 68.45s/it]2024-09-23 20:51:59,509 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:52:05,760 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:52:12,586 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:52:18,936 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:52:25,860 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:52:32,255 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:52:39,156 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:52:45,390 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:52:52,214 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:52:58,531 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  87%|████████▋ | 13/15 [14:24<02:15, 67.70s/it]2024-09-23 20:53:05,493 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:53:12,064 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:53:18,995 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:53:26,051 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:53:33,168 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:53:40,363 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:53:47,260 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:53:53,799 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:54:00,808 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:54:08,003 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  93%|█████████▎| 14/15 [15:34<01:08, 68.42s/it]2024-09-23 20:54:15,759 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:54:23,681 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:54:30,881 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:54:38,143 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:54:45,215 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:54:52,526 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:54:59,411 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:55:06,483 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 20:55:13,397 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:55:20,598 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning: 100%|██████████| 15/15 [16:46<00:00, 67.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: max_depth: 3, n_estimators: 200, learning_rate: 0.1, subsample: 0.8, colsample_bytree: 0.8\n",
      "Best areaUnderROC: 0.9490\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 20:55:25,413 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 20:55:31,385 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Evaluation on Test Data areaUnderROC: 0.9628\n",
      "Final Accuracy: 0.9628, Precision: 0.9462, Recall: 0.9814\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000084_001-000084_MG_BL_Series-1_Image-1-0.png', 'auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000245_003-000245_MG_BL_Series-1008_Image-1-0.png', 'hcs_003-000259_003-000259_MG_BL_Series-2_Image-1-1.png', 'hcs_003-001181_003-001181_MG_BL_Series-1694_Image-1694-1.png']\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = SparkXGBClassifier(label_col=\"class\", features_col=\"features\", use_gpu=False) \n",
    "\n",
    "# Set up the parameter grid for hyperparameter tuning\n",
    "xgb_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(xgb_classifier.max_depth, [3, 6, 9, 12, 15]) \\\n",
    "    .addGrid(xgb_classifier.n_estimators, [50, 100, 200]) \\\n",
    "    .addGrid(xgb_classifier.learning_rate, [0.1]) \\\n",
    "    .addGrid(xgb_classifier.subsample, [0.8]) \\\n",
    "    .addGrid(xgb_classifier.colsample_bytree, [0.8]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(xgb_classifier, file_name, xgb_param_grid, is_tree=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "Number of rows with NaN values: 1\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2254 (80.73%), Test size: 538 rows (19.27%)\n",
      "Class distribution in train_df: [0: 1126 (49.96%), 1: 1128 (50.04%)] | test_df: [0: 269 (50.00%), 1: 269 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 510 rows (22.63%) | Fold 2: 414 rows (18.37%) | Fold 3: 417 rows (18.50%) | Fold 4: 474 rows (21.03%) | Fold 5: 439 rows (19.48%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 12/12 [06:03<00:00, 30.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: maxIter: 500, regParam: 0.01, tol: 0.0001\n",
      "Best areaUnderROC: 0.9411\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.9572\n",
      "Final Accuracy: 0.9572, Precision: 0.9241, Recall: 0.9963\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000245_003-000245_MG_BL_Series-1008_Image-1-0.png', 'hcs_003-000247_003-000247_MG_BL_Series-1005_Image-1-1.png', 'hcs_003-001862_003-001862_MG_BL_Series-1001_Image-1001-0.png', 'hcs_003-001862_003-001862_MG_BL_Series-1001_Image-1003-0.png']\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC with Standard Scaler\n",
    "svc = LinearSVC(labelCol=\"class\", featuresCol=\"features\")\n",
    "\n",
    "# Set up the parameter grid for hyperparameter tuning\n",
    "svc_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(svc.maxIter, [100, 500, 1000]) \\\n",
    "    .addGrid(svc.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(svc.tol, [1e-4,  1e-2]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function with the XGBoost classifier\n",
    "model = main(svc, file_name, svc_param_grid, is_tree=False, use_standard_scaler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 256 - with full mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'features_256_full_mask.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2286 (81.82%), Test size: 508 rows (18.18%)\n",
      "Class distribution in train_df: [0: 1143 (50.00%), 1: 1143 (50.00%)] | test_df: [0: 254 (50.00%), 1: 254 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 494 rows (21.61%) | Fold 2: 398 rows (17.41%) | Fold 3: 448 rows (19.60%) | Fold 4: 480 rows (21.00%) | Fold 5: 466 rows (20.38%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 8/8 [02:35<00:00, 19.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: maxDepth: 5, maxBins: 64\n",
      "Best areaUnderROC: 0.7221\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.7244\n",
      "Final Accuracy: 0.7244, Precision: 0.7280, Recall: 0.7165\n",
      "\n",
      "Sorted Feature Importances:\n",
      "Feature: original_firstorder_90Percentile, Importance: 0.7217\n",
      "Feature: original_firstorder_Variance, Importance: 0.0899\n",
      "Feature: original_firstorder_Kurtosis, Importance: 0.0313\n",
      "Feature: original_firstorder_MeanAbsoluteDeviation, Importance: 0.0222\n",
      "Feature: original_glcm_ClusterProminence, Importance: 0.0189\n",
      "Feature: original_glszm_LargeAreaEmphasis, Importance: 0.0148\n",
      "Feature: original_gldm_SmallDependenceHighGrayLevelEmphasis, Importance: 0.0148\n",
      "Feature: original_glrlm_LongRunHighGrayLevelEmphasis, Importance: 0.0127\n",
      "Feature: original_glrlm_ShortRunHighGrayLevelEmphasis, Importance: 0.0123\n",
      "Feature: original_glszm_SmallAreaLowGrayLevelEmphasis, Importance: 0.0111\n",
      "Feature: original_glszm_ZoneEntropy, Importance: 0.0109\n",
      "Feature: original_glrlm_RunLengthNonUniformity, Importance: 0.0101\n",
      "Feature: original_glrlm_HighGrayLevelRunEmphasis, Importance: 0.0098\n",
      "Feature: original_firstorder_InterquartileRange, Importance: 0.0064\n",
      "Feature: original_glcm_JointEntropy, Importance: 0.0060\n",
      "Feature: original_glszm_SmallAreaHighGrayLevelEmphasis, Importance: 0.0039\n",
      "Feature: original_glcm_Autocorrelation, Importance: 0.0033\n",
      "Feature: original_firstorder_10Percentile, Importance: 0.0000\n",
      "Feature: original_firstorder_Energy, Importance: 0.0000\n",
      "Feature: original_firstorder_Entropy, Importance: 0.0000\n",
      "Feature: original_firstorder_Maximum, Importance: 0.0000\n",
      "Feature: original_firstorder_Mean, Importance: 0.0000\n",
      "Feature: original_firstorder_Median, Importance: 0.0000\n",
      "Feature: original_firstorder_Minimum, Importance: 0.0000\n",
      "Feature: original_firstorder_Range, Importance: 0.0000\n",
      "Feature: original_firstorder_RobustMeanAbsoluteDeviation, Importance: 0.0000\n",
      "Feature: original_firstorder_RootMeanSquared, Importance: 0.0000\n",
      "Feature: original_firstorder_Skewness, Importance: 0.0000\n",
      "Feature: original_firstorder_TotalEnergy, Importance: 0.0000\n",
      "Feature: original_firstorder_Uniformity, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterShade, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterTendency, Importance: 0.0000\n",
      "Feature: original_glcm_Contrast, Importance: 0.0000\n",
      "Feature: original_glcm_Correlation, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceAverage, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceVariance, Importance: 0.0000\n",
      "Feature: original_glcm_Id, Importance: 0.0000\n",
      "Feature: original_glcm_Idm, Importance: 0.0000\n",
      "Feature: original_glcm_Idmn, Importance: 0.0000\n",
      "Feature: original_glcm_Idn, Importance: 0.0000\n",
      "Feature: original_glcm_Imc1, Importance: 0.0000\n",
      "Feature: original_glcm_Imc2, Importance: 0.0000\n",
      "Feature: original_glcm_InverseVariance, Importance: 0.0000\n",
      "Feature: original_glcm_JointAverage, Importance: 0.0000\n",
      "Feature: original_glcm_JointEnergy, Importance: 0.0000\n",
      "Feature: original_glcm_MCC, Importance: 0.0000\n",
      "Feature: original_glcm_MaximumProbability, Importance: 0.0000\n",
      "Feature: original_glcm_SumAverage, Importance: 0.0000\n",
      "Feature: original_glcm_SumEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_SumSquares, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceEntropy, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceNonUniformity, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceVariance, Importance: 0.0000\n",
      "Feature: original_gldm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_gldm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_gldm_HighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LowGrayLevelRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_RunEntropy, Importance: 0.0000\n",
      "Feature: original_glrlm_RunLengthNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glrlm_RunPercentage, Importance: 0.0000\n",
      "Feature: original_glrlm_RunVariance, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_glszm_HighGrayLevelZoneEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LowGrayLevelZoneEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SizeZoneNonUniformity, Importance: 0.0000\n",
      "Feature: original_glszm_SizeZoneNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_ZonePercentage, Importance: 0.0000\n",
      "Feature: original_glszm_ZoneVariance, Importance: 0.0000\n",
      "Feature: original_ngtdm_Busyness, Importance: 0.0000\n",
      "Feature: original_ngtdm_Coarseness, Importance: 0.0000\n",
      "Feature: original_ngtdm_Complexity, Importance: 0.0000\n",
      "Feature: original_ngtdm_Contrast, Importance: 0.0000\n",
      "Feature: original_ngtdm_Strength, Importance: 0.0000\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000074_001-000074_MG_TP1_Series-4_Image-1-1.png', 'auth_001-000084_001-000084_MG_BL_Series-1_Image-1-1.png', 'auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-1.png']\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(labelCol = \"class\", featuresCol = \"features\")\n",
    "\n",
    "# Set up parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(decision_tree.maxDepth, [5, 10, 15, 20]) \\\n",
    "    .addGrid(decision_tree.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(decision_tree, file_name, param_grid, is_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2286 (81.82%), Test size: 508 rows (18.18%)\n",
      "Class distribution in train_df: [0: 1143 (50.00%), 1: 1143 (50.00%)] | test_df: [0: 254 (50.00%), 1: 254 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 494 rows (21.61%) | Fold 2: 398 rows (17.41%) | Fold 3: 448 rows (19.60%) | Fold 4: 480 rows (21.00%) | Fold 5: 466 rows (20.38%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 36/36 [36:55<00:00, 61.54s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: numTrees: 150, maxDepth: 5, maxBins: 64, featureSubsetStrategy: auto\n",
      "Best areaUnderROC: 0.7388\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.7480\n",
      "Final Accuracy: 0.7480, Precision: 0.7582, Recall: 0.7283\n",
      "\n",
      "Sorted Feature Importances:\n",
      "Feature: original_firstorder_90Percentile, Importance: 0.1140\n",
      "Feature: original_firstorder_MeanAbsoluteDeviation, Importance: 0.0955\n",
      "Feature: original_firstorder_Variance, Importance: 0.0943\n",
      "Feature: original_firstorder_RobustMeanAbsoluteDeviation, Importance: 0.0732\n",
      "Feature: original_firstorder_InterquartileRange, Importance: 0.0702\n",
      "Feature: original_firstorder_Energy, Importance: 0.0651\n",
      "Feature: original_firstorder_TotalEnergy, Importance: 0.0648\n",
      "Feature: original_firstorder_Mean, Importance: 0.0506\n",
      "Feature: original_firstorder_RootMeanSquared, Importance: 0.0450\n",
      "Feature: original_firstorder_Median, Importance: 0.0304\n",
      "Feature: original_firstorder_Kurtosis, Importance: 0.0175\n",
      "Feature: original_firstorder_Skewness, Importance: 0.0156\n",
      "Feature: original_glszm_LargeAreaEmphasis, Importance: 0.0104\n",
      "Feature: original_glrlm_LongRunLowGrayLevelEmphasis, Importance: 0.0093\n",
      "Feature: original_glszm_GrayLevelNonUniformity, Importance: 0.0082\n",
      "Feature: original_glszm_ZoneVariance, Importance: 0.0080\n",
      "Feature: original_glszm_SizeZoneNonUniformity, Importance: 0.0067\n",
      "Feature: original_firstorder_10Percentile, Importance: 0.0056\n",
      "Feature: original_glcm_Imc2, Importance: 0.0054\n",
      "Feature: original_glcm_ClusterShade, Importance: 0.0054\n",
      "Feature: original_gldm_DependenceNonUniformityNormalized, Importance: 0.0054\n",
      "Feature: original_glcm_MCC, Importance: 0.0052\n",
      "Feature: original_glcm_Imc1, Importance: 0.0051\n",
      "Feature: original_glcm_ClusterProminence, Importance: 0.0049\n",
      "Feature: original_gldm_SmallDependenceLowGrayLevelEmphasis, Importance: 0.0046\n",
      "Feature: original_glrlm_RunEntropy, Importance: 0.0045\n",
      "Feature: original_glrlm_RunLengthNonUniformity, Importance: 0.0044\n",
      "Feature: original_glrlm_RunVariance, Importance: 0.0044\n",
      "Feature: original_glcm_Correlation, Importance: 0.0044\n",
      "Feature: original_glszm_SmallAreaHighGrayLevelEmphasis, Importance: 0.0044\n",
      "Feature: original_glrlm_HighGrayLevelRunEmphasis, Importance: 0.0043\n",
      "Feature: original_glszm_HighGrayLevelZoneEmphasis, Importance: 0.0042\n",
      "Feature: original_glcm_SumAverage, Importance: 0.0042\n",
      "Feature: original_glszm_SmallAreaLowGrayLevelEmphasis, Importance: 0.0041\n",
      "Feature: original_gldm_SmallDependenceEmphasis, Importance: 0.0040\n",
      "Feature: original_glszm_SizeZoneNonUniformityNormalized, Importance: 0.0040\n",
      "Feature: original_glrlm_ShortRunLowGrayLevelEmphasis, Importance: 0.0039\n",
      "Feature: original_glszm_GrayLevelVariance, Importance: 0.0039\n",
      "Feature: original_glszm_SmallAreaEmphasis, Importance: 0.0039\n",
      "Feature: original_glrlm_LowGrayLevelRunEmphasis, Importance: 0.0039\n",
      "Feature: original_gldm_DependenceNonUniformity, Importance: 0.0037\n",
      "Feature: original_glszm_ZonePercentage, Importance: 0.0037\n",
      "Feature: original_glszm_LowGrayLevelZoneEmphasis, Importance: 0.0036\n",
      "Feature: original_glcm_JointAverage, Importance: 0.0036\n",
      "Feature: original_glcm_SumEntropy, Importance: 0.0036\n",
      "Feature: original_gldm_LowGrayLevelEmphasis, Importance: 0.0036\n",
      "Feature: original_glrlm_ShortRunEmphasis, Importance: 0.0036\n",
      "Feature: original_glszm_LargeAreaHighGrayLevelEmphasis, Importance: 0.0035\n",
      "Feature: original_glszm_GrayLevelNonUniformityNormalized, Importance: 0.0034\n",
      "Feature: original_glcm_Autocorrelation, Importance: 0.0032\n",
      "Feature: original_glrlm_LongRunHighGrayLevelEmphasis, Importance: 0.0031\n",
      "Feature: original_gldm_HighGrayLevelEmphasis, Importance: 0.0030\n",
      "Feature: original_glcm_ClusterTendency, Importance: 0.0029\n",
      "Feature: original_gldm_LargeDependenceLowGrayLevelEmphasis, Importance: 0.0029\n",
      "Feature: original_gldm_SmallDependenceHighGrayLevelEmphasis, Importance: 0.0028\n",
      "Feature: original_gldm_GrayLevelVariance, Importance: 0.0027\n",
      "Feature: original_glrlm_LongRunEmphasis, Importance: 0.0027\n",
      "Feature: original_gldm_DependenceEntropy, Importance: 0.0027\n",
      "Feature: original_gldm_DependenceVariance, Importance: 0.0026\n",
      "Feature: original_glszm_LargeAreaLowGrayLevelEmphasis, Importance: 0.0026\n",
      "Feature: original_glcm_JointEnergy, Importance: 0.0025\n",
      "Feature: original_firstorder_Entropy, Importance: 0.0024\n",
      "Feature: original_glrlm_ShortRunHighGrayLevelEmphasis, Importance: 0.0024\n",
      "Feature: original_glrlm_GrayLevelVariance, Importance: 0.0024\n",
      "Feature: original_glrlm_GrayLevelNonUniformityNormalized, Importance: 0.0024\n",
      "Feature: original_gldm_GrayLevelNonUniformity, Importance: 0.0023\n",
      "Feature: original_glrlm_RunLengthNonUniformityNormalized, Importance: 0.0023\n",
      "Feature: original_glcm_SumSquares, Importance: 0.0023\n",
      "Feature: original_gldm_LargeDependenceHighGrayLevelEmphasis, Importance: 0.0022\n",
      "Feature: original_glcm_MaximumProbability, Importance: 0.0022\n",
      "Feature: original_glrlm_GrayLevelNonUniformity, Importance: 0.0021\n",
      "Feature: original_glcm_DifferenceVariance, Importance: 0.0021\n",
      "Feature: original_glcm_InverseVariance, Importance: 0.0021\n",
      "Feature: original_glszm_ZoneEntropy, Importance: 0.0020\n",
      "Feature: original_ngtdm_Complexity, Importance: 0.0019\n",
      "Feature: original_glcm_JointEntropy, Importance: 0.0017\n",
      "Feature: original_firstorder_Uniformity, Importance: 0.0017\n",
      "Feature: original_glcm_Idm, Importance: 0.0017\n",
      "Feature: original_gldm_LargeDependenceEmphasis, Importance: 0.0016\n",
      "Feature: original_glcm_Contrast, Importance: 0.0015\n",
      "Feature: original_ngtdm_Strength, Importance: 0.0015\n",
      "Feature: original_ngtdm_Busyness, Importance: 0.0015\n",
      "Feature: original_glcm_DifferenceAverage, Importance: 0.0014\n",
      "Feature: original_ngtdm_Coarseness, Importance: 0.0014\n",
      "Feature: original_glrlm_RunPercentage, Importance: 0.0013\n",
      "Feature: original_ngtdm_Contrast, Importance: 0.0011\n",
      "Feature: original_glcm_Idmn, Importance: 0.0010\n",
      "Feature: original_glcm_Id, Importance: 0.0010\n",
      "Feature: original_glcm_Idn, Importance: 0.0009\n",
      "Feature: original_glcm_DifferenceEntropy, Importance: 0.0004\n",
      "Feature: original_firstorder_Maximum, Importance: 0.0003\n",
      "Feature: original_firstorder_Range, Importance: 0.0003\n",
      "Feature: original_firstorder_Minimum, Importance: 0.0002\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000074_001-000074_MG_TP1_Series-4_Image-1-1.png', 'auth_001-000084_001-000084_MG_BL_Series-1_Image-1-0.png', 'hcs_003-000024_003-000024_MG_BL_Series-1003_Image-1003-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-1.png']\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(labelCol=\"class\", featuresCol=\"features\")\n",
    "\n",
    "# Set up parameter grid for hyperparameter tuning\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(random_forest.numTrees, [50, 150]) \\\n",
    "    .addGrid(random_forest.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(random_forest.maxBins, [32, 64, 128]) \\\n",
    "    .addGrid(random_forest.featureSubsetStrategy, ['auto', 'sqrt']) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(random_forest, file_name, rf_param_grid, is_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2286 (81.82%), Test size: 508 rows (18.18%)\n",
      "Class distribution in train_df: [0: 1143 (50.00%), 1: 1143 (50.00%)] | test_df: [0: 254 (50.00%), 1: 254 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 494 rows (21.61%) | Fold 2: 398 rows (17.41%) | Fold 3: 448 rows (19.60%) | Fold 4: 480 rows (21.00%) | Fold 5: 466 rows (20.38%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning:   0%|          | 0/15 [00:00<?, ?it/s]2024-09-23 21:43:12,626 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:43:17,659 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:43:23,334 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:43:28,491 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:43:35,835 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:43:41,647 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:43:48,806 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:43:54,019 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:44:00,001 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:44:05,646 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:   7%|▋         | 1/15 [00:59<13:55, 59.65s/it]2024-09-23 21:44:12,891 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:44:18,737 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:44:26,033 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:44:31,665 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:44:37,934 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:44:42,875 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:44:48,401 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:44:54,037 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:45:01,353 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:45:07,188 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  13%|█▎        | 2/15 [02:01<13:15, 61.22s/it]2024-09-23 21:45:16,664 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:45:24,809 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:45:33,601 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:45:40,125 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:45:46,785 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:45:52,373 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:45:57,693 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:46:02,885 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:46:09,509 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:46:15,409 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  20%|██        | 3/15 [03:09<12:47, 63.97s/it]2024-09-23 21:46:22,305 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:46:28,190 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:46:35,081 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:46:40,985 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:46:47,847 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:46:53,820 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:47:00,161 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:47:05,429 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:47:11,693 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:47:17,636 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  27%|██▋       | 4/15 [04:11<11:36, 63.31s/it]2024-09-23 21:47:24,611 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:47:31,016 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:47:38,002 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:47:44,454 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:47:51,377 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:47:57,817 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:48:03,196 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:48:09,041 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:48:15,204 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:48:21,620 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  33%|███▎      | 5/15 [05:15<10:35, 63.56s/it]2024-09-23 21:48:28,629 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:48:36,050 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:48:43,146 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:48:50,564 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:48:57,510 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:49:04,985 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:49:12,099 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:49:19,549 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:49:26,524 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:49:33,137 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  40%|████      | 6/15 [06:25<09:53, 65.90s/it]2024-09-23 21:49:38,464 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:49:45,189 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:49:52,163 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:49:58,970 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:50:05,880 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:50:12,588 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:50:19,482 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:50:26,202 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:50:33,101 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:50:39,835 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  47%|████▋     | 7/15 [07:33<08:51, 66.49s/it]2024-09-23 21:50:46,871 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:50:54,445 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:51:00,789 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:51:08,084 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:51:15,010 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:51:22,662 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:51:29,567 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:51:37,156 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:51:43,551 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:51:50,420 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  53%|█████▎    | 8/15 [08:44<07:54, 67.82s/it]2024-09-23 21:51:57,392 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:52:06,463 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:52:13,472 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:52:22,511 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:52:29,512 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:52:38,511 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:52:45,545 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:52:54,387 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:52:59,841 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:53:08,110 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  60%|██████    | 9/15 [10:02<07:05, 70.92s/it]2024-09-23 21:53:15,157 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:53:22,519 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:53:29,428 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:53:36,597 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:53:43,508 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:53:50,759 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:53:57,781 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:54:05,195 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:54:12,197 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:54:19,663 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  67%|██████▋   | 10/15 [11:13<05:55, 71.07s/it]2024-09-23 21:54:26,259 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:54:33,822 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:54:40,875 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:54:49,271 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:54:56,333 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:55:04,913 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:55:12,381 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:55:21,246 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:55:28,403 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:55:36,835 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  73%|███████▎  | 11/15 [12:29<04:50, 72.68s/it]2024-09-23 21:55:42,308 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:55:51,162 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:55:58,465 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:56:08,299 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:56:15,330 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:56:25,234 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:56:32,323 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:56:42,270 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:56:49,984 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:57:00,466 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  80%|████████  | 12/15 [13:54<03:48, 76.22s/it]2024-09-23 21:57:06,978 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:57:14,489 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:57:21,560 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:57:29,269 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:57:36,321 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:57:43,983 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:57:50,307 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:57:57,019 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:58:03,803 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:58:11,591 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  87%|████████▋ | 13/15 [15:05<02:29, 74.77s/it]2024-09-23 21:58:18,639 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:58:27,366 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:58:34,318 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:58:43,331 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:58:50,338 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:58:59,156 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:59:05,305 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:59:13,485 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:59:20,570 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:59:29,511 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  93%|█████████▎| 14/15 [16:23<01:15, 75.72s/it]2024-09-23 21:59:36,585 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 21:59:46,651 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 21:59:53,655 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:00:03,984 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:00:09,722 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:00:19,749 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:00:27,353 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:00:37,671 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:00:44,791 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:00:55,158 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning: 100%|██████████| 15/15 [17:49<00:00, 71.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: max_depth: 6, n_estimators: 50, learning_rate: 0.1, subsample: 0.8, colsample_bytree: 0.8\n",
      "Best areaUnderROC: 0.7494\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 22:01:00,139 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:01:06,301 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Evaluation on Test Data areaUnderROC: 0.7480\n",
      "Final Accuracy: 0.7480, Precision: 0.7669, Recall: 0.7126\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000074_001-000074_MG_TP1_Series-4_Image-1-1.png', 'auth_001-000084_001-000084_MG_BL_Series-1_Image-1-0.png', 'auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000024_003-000024_MG_BL_Series-1003_Image-1003-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-0.png']\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = SparkXGBClassifier(label_col=\"class\", features_col=\"features\", use_gpu=False) \n",
    "\n",
    "# Set up the parameter grid for hyperparameter tuning\n",
    "xgb_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(xgb_classifier.max_depth, [3, 6, 9, 12, 15]) \\\n",
    "    .addGrid(xgb_classifier.n_estimators, [50, 100, 200]) \\\n",
    "    .addGrid(xgb_classifier.learning_rate, [0.1]) \\\n",
    "    .addGrid(xgb_classifier.subsample, [0.8]) \\\n",
    "    .addGrid(xgb_classifier.colsample_bytree, [0.8]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(xgb_classifier, file_name, xgb_param_grid, is_tree=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2286 (81.82%), Test size: 508 rows (18.18%)\n",
      "Class distribution in train_df: [0: 1143 (50.00%), 1: 1143 (50.00%)] | test_df: [0: 254 (50.00%), 1: 254 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 494 rows (21.61%) | Fold 2: 398 rows (17.41%) | Fold 3: 448 rows (19.60%) | Fold 4: 480 rows (21.00%) | Fold 5: 466 rows (20.38%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 12/12 [06:30<00:00, 32.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: maxIter: 100, regParam: 0.01, tol: 0.01\n",
      "Best areaUnderROC: 0.7380\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.7343\n",
      "Final Accuracy: 0.7343, Precision: 0.7196, Recall: 0.7677\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000061_001-000061_MG_BL_Series-8_Image-1-1.png', 'hcs_003-000024_003-000024_MG_BL_Series-1003_Image-1003-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-1.png', 'hcs_003-000029_003-000029_MG_BL_Series-1002_Image-1002-0.png']\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC with Standard Scaler\n",
    "svc = LinearSVC(labelCol=\"class\", featuresCol=\"features\")\n",
    "\n",
    "# Set up the parameter grid for hyperparameter tuning\n",
    "svc_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(svc.maxIter, [100, 500, 1000]) \\\n",
    "    .addGrid(svc.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(svc.tol, [1e-4,  1e-2]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function with the XGBoost classifier\n",
    "model = main(svc, file_name, svc_param_grid, is_tree=False, use_standard_scaler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 128 - with lesion mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'features_128_lesion_mask.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2261 (80.84%), Test size: 536 rows (19.16%)\n",
      "Class distribution in train_df: [0: 1130 (49.98%), 1: 1131 (50.02%)] | test_df: [0: 268 (50.00%), 1: 268 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 430 rows (19.02%) | Fold 2: 448 rows (19.81%) | Fold 3: 428 rows (18.93%) | Fold 4: 496 rows (21.94%) | Fold 5: 459 rows (20.30%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 8/8 [02:26<00:00, 18.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: maxDepth: 5, maxBins: 64\n",
      "Best areaUnderROC: 0.8538\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.8937\n",
      "Final Accuracy: 0.8937, Precision: 0.8754, Recall: 0.9179\n",
      "\n",
      "Sorted Feature Importances:\n",
      "Feature: original_firstorder_RootMeanSquared, Importance: 0.6193\n",
      "Feature: original_gldm_GrayLevelNonUniformity, Importance: 0.1103\n",
      "Feature: original_glrlm_RunLengthNonUniformity, Importance: 0.1018\n",
      "Feature: original_firstorder_90Percentile, Importance: 0.0709\n",
      "Feature: original_glszm_SmallAreaLowGrayLevelEmphasis, Importance: 0.0230\n",
      "Feature: original_gldm_DependenceNonUniformity, Importance: 0.0157\n",
      "Feature: original_firstorder_Skewness, Importance: 0.0137\n",
      "Feature: original_glcm_Imc1, Importance: 0.0125\n",
      "Feature: original_firstorder_10Percentile, Importance: 0.0094\n",
      "Feature: original_glszm_SizeZoneNonUniformityNormalized, Importance: 0.0063\n",
      "Feature: original_glcm_ClusterProminence, Importance: 0.0058\n",
      "Feature: original_glrlm_GrayLevelNonUniformity, Importance: 0.0056\n",
      "Feature: original_glcm_JointEnergy, Importance: 0.0032\n",
      "Feature: original_firstorder_Energy, Importance: 0.0024\n",
      "Feature: original_firstorder_Mean, Importance: 0.0003\n",
      "Feature: original_firstorder_Entropy, Importance: 0.0000\n",
      "Feature: original_firstorder_InterquartileRange, Importance: 0.0000\n",
      "Feature: original_firstorder_Kurtosis, Importance: 0.0000\n",
      "Feature: original_firstorder_Maximum, Importance: 0.0000\n",
      "Feature: original_firstorder_MeanAbsoluteDeviation, Importance: 0.0000\n",
      "Feature: original_firstorder_Median, Importance: 0.0000\n",
      "Feature: original_firstorder_Minimum, Importance: 0.0000\n",
      "Feature: original_firstorder_Range, Importance: 0.0000\n",
      "Feature: original_firstorder_RobustMeanAbsoluteDeviation, Importance: 0.0000\n",
      "Feature: original_firstorder_TotalEnergy, Importance: 0.0000\n",
      "Feature: original_firstorder_Uniformity, Importance: 0.0000\n",
      "Feature: original_firstorder_Variance, Importance: 0.0000\n",
      "Feature: original_glcm_Autocorrelation, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterShade, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterTendency, Importance: 0.0000\n",
      "Feature: original_glcm_Contrast, Importance: 0.0000\n",
      "Feature: original_glcm_Correlation, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceAverage, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceVariance, Importance: 0.0000\n",
      "Feature: original_glcm_Id, Importance: 0.0000\n",
      "Feature: original_glcm_Idm, Importance: 0.0000\n",
      "Feature: original_glcm_Idmn, Importance: 0.0000\n",
      "Feature: original_glcm_Idn, Importance: 0.0000\n",
      "Feature: original_glcm_Imc2, Importance: 0.0000\n",
      "Feature: original_glcm_InverseVariance, Importance: 0.0000\n",
      "Feature: original_glcm_JointAverage, Importance: 0.0000\n",
      "Feature: original_glcm_JointEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_MCC, Importance: 0.0000\n",
      "Feature: original_glcm_MaximumProbability, Importance: 0.0000\n",
      "Feature: original_glcm_SumAverage, Importance: 0.0000\n",
      "Feature: original_glcm_SumEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_SumSquares, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceEntropy, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceVariance, Importance: 0.0000\n",
      "Feature: original_gldm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_gldm_HighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_glrlm_HighGrayLevelRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LowGrayLevelRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_RunEntropy, Importance: 0.0000\n",
      "Feature: original_glrlm_RunLengthNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glrlm_RunPercentage, Importance: 0.0000\n",
      "Feature: original_glrlm_RunVariance, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_glszm_HighGrayLevelZoneEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LowGrayLevelZoneEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SizeZoneNonUniformity, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_ZoneEntropy, Importance: 0.0000\n",
      "Feature: original_glszm_ZonePercentage, Importance: 0.0000\n",
      "Feature: original_glszm_ZoneVariance, Importance: 0.0000\n",
      "Feature: original_ngtdm_Busyness, Importance: 0.0000\n",
      "Feature: original_ngtdm_Coarseness, Importance: 0.0000\n",
      "Feature: original_ngtdm_Complexity, Importance: 0.0000\n",
      "Feature: original_ngtdm_Contrast, Importance: 0.0000\n",
      "Feature: original_ngtdm_Strength, Importance: 0.0000\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000071_001-000071_MG_TP3_Series-4_Image-1-0.png', 'auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000245_003-000245_MG_BL_Series-1008_Image-1-0.png', 'hcs_003-000252_003-000252_MG_TP3_Series-2_Image-1-1.png', 'hcs_003-000252_003-000252_MG_TP3_Series-4_Image-1-1.png']\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(labelCol = \"class\", featuresCol = \"features\")\n",
    "\n",
    "# Set up parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(decision_tree.maxDepth, [5, 10, 15, 20]) \\\n",
    "    .addGrid(decision_tree.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(decision_tree, file_name, param_grid, is_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2261 (80.84%), Test size: 536 rows (19.16%)\n",
      "Class distribution in train_df: [0: 1130 (49.98%), 1: 1131 (50.02%)] | test_df: [0: 268 (50.00%), 1: 268 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 430 rows (19.02%) | Fold 2: 448 rows (19.81%) | Fold 3: 428 rows (18.93%) | Fold 4: 496 rows (21.94%) | Fold 5: 459 rows (20.30%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 36/36 [25:43<00:00, 42.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: numTrees: 50, maxDepth: 10, maxBins: 32, featureSubsetStrategy: auto\n",
      "Best areaUnderROC: 0.8697\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.8955\n",
      "Final Accuracy: 0.8955, Precision: 0.9015, Recall: 0.8881\n",
      "\n",
      "Sorted Feature Importances:\n",
      "Feature: original_firstorder_Median, Importance: 0.0918\n",
      "Feature: original_firstorder_Skewness, Importance: 0.0646\n",
      "Feature: original_glrlm_RunLengthNonUniformity, Importance: 0.0566\n",
      "Feature: original_glrlm_GrayLevelNonUniformity, Importance: 0.0536\n",
      "Feature: original_firstorder_Mean, Importance: 0.0501\n",
      "Feature: original_firstorder_RootMeanSquared, Importance: 0.0494\n",
      "Feature: original_firstorder_90Percentile, Importance: 0.0479\n",
      "Feature: original_glszm_SizeZoneNonUniformity, Importance: 0.0433\n",
      "Feature: original_gldm_SmallDependenceLowGrayLevelEmphasis, Importance: 0.0341\n",
      "Feature: original_gldm_GrayLevelNonUniformity, Importance: 0.0340\n",
      "Feature: original_ngtdm_Busyness, Importance: 0.0276\n",
      "Feature: original_glszm_GrayLevelNonUniformity, Importance: 0.0267\n",
      "Feature: original_gldm_DependenceNonUniformity, Importance: 0.0222\n",
      "Feature: original_firstorder_TotalEnergy, Importance: 0.0154\n",
      "Feature: original_firstorder_10Percentile, Importance: 0.0143\n",
      "Feature: original_firstorder_RobustMeanAbsoluteDeviation, Importance: 0.0130\n",
      "Feature: original_firstorder_Energy, Importance: 0.0126\n",
      "Feature: original_firstorder_InterquartileRange, Importance: 0.0118\n",
      "Feature: original_firstorder_Variance, Importance: 0.0115\n",
      "Feature: original_firstorder_Minimum, Importance: 0.0115\n",
      "Feature: original_glrlm_RunLengthNonUniformityNormalized, Importance: 0.0109\n",
      "Feature: original_firstorder_MeanAbsoluteDeviation, Importance: 0.0107\n",
      "Feature: original_ngtdm_Strength, Importance: 0.0104\n",
      "Feature: original_glrlm_ShortRunLowGrayLevelEmphasis, Importance: 0.0098\n",
      "Feature: original_ngtdm_Coarseness, Importance: 0.0094\n",
      "Feature: original_glszm_LowGrayLevelZoneEmphasis, Importance: 0.0089\n",
      "Feature: original_glrlm_HighGrayLevelRunEmphasis, Importance: 0.0087\n",
      "Feature: original_glszm_GrayLevelVariance, Importance: 0.0085\n",
      "Feature: original_firstorder_Kurtosis, Importance: 0.0083\n",
      "Feature: original_glszm_ZonePercentage, Importance: 0.0083\n",
      "Feature: original_glrlm_GrayLevelVariance, Importance: 0.0076\n",
      "Feature: original_glcm_MCC, Importance: 0.0075\n",
      "Feature: original_glszm_SmallAreaEmphasis, Importance: 0.0072\n",
      "Feature: original_glrlm_RunEntropy, Importance: 0.0070\n",
      "Feature: original_glcm_Correlation, Importance: 0.0068\n",
      "Feature: original_glcm_Imc1, Importance: 0.0068\n",
      "Feature: original_glszm_SmallAreaLowGrayLevelEmphasis, Importance: 0.0063\n",
      "Feature: original_glszm_HighGrayLevelZoneEmphasis, Importance: 0.0062\n",
      "Feature: original_glszm_SmallAreaHighGrayLevelEmphasis, Importance: 0.0060\n",
      "Feature: original_glcm_Idn, Importance: 0.0059\n",
      "Feature: original_glszm_SizeZoneNonUniformityNormalized, Importance: 0.0058\n",
      "Feature: original_gldm_LargeDependenceLowGrayLevelEmphasis, Importance: 0.0054\n",
      "Feature: original_glszm_ZoneEntropy, Importance: 0.0053\n",
      "Feature: original_glrlm_RunVariance, Importance: 0.0052\n",
      "Feature: original_glcm_DifferenceVariance, Importance: 0.0050\n",
      "Feature: original_glrlm_ShortRunHighGrayLevelEmphasis, Importance: 0.0049\n",
      "Feature: original_glrlm_LongRunLowGrayLevelEmphasis, Importance: 0.0049\n",
      "Feature: original_glszm_GrayLevelNonUniformityNormalized, Importance: 0.0047\n",
      "Feature: original_glszm_LargeAreaEmphasis, Importance: 0.0044\n",
      "Feature: original_glcm_Imc2, Importance: 0.0042\n",
      "Feature: original_glszm_LargeAreaLowGrayLevelEmphasis, Importance: 0.0039\n",
      "Feature: original_glszm_LargeAreaHighGrayLevelEmphasis, Importance: 0.0038\n",
      "Feature: original_glrlm_ShortRunEmphasis, Importance: 0.0037\n",
      "Feature: original_gldm_SmallDependenceHighGrayLevelEmphasis, Importance: 0.0035\n",
      "Feature: original_glrlm_LowGrayLevelRunEmphasis, Importance: 0.0035\n",
      "Feature: original_glcm_ClusterShade, Importance: 0.0032\n",
      "Feature: original_glcm_JointAverage, Importance: 0.0032\n",
      "Feature: original_glcm_SumAverage, Importance: 0.0031\n",
      "Feature: original_gldm_HighGrayLevelEmphasis, Importance: 0.0030\n",
      "Feature: original_firstorder_Entropy, Importance: 0.0030\n",
      "Feature: original_glrlm_LongRunHighGrayLevelEmphasis, Importance: 0.0030\n",
      "Feature: original_glrlm_LongRunEmphasis, Importance: 0.0028\n",
      "Feature: original_firstorder_Uniformity, Importance: 0.0027\n",
      "Feature: original_gldm_LowGrayLevelEmphasis, Importance: 0.0027\n",
      "Feature: original_glcm_SumEntropy, Importance: 0.0026\n",
      "Feature: original_glszm_ZoneVariance, Importance: 0.0026\n",
      "Feature: original_glcm_MaximumProbability, Importance: 0.0025\n",
      "Feature: original_gldm_SmallDependenceEmphasis, Importance: 0.0025\n",
      "Feature: original_glcm_Autocorrelation, Importance: 0.0024\n",
      "Feature: original_gldm_DependenceEntropy, Importance: 0.0024\n",
      "Feature: original_glcm_JointEnergy, Importance: 0.0024\n",
      "Feature: original_glcm_SumSquares, Importance: 0.0023\n",
      "Feature: original_glrlm_GrayLevelNonUniformityNormalized, Importance: 0.0021\n",
      "Feature: original_glcm_ClusterTendency, Importance: 0.0021\n",
      "Feature: original_firstorder_Range, Importance: 0.0021\n",
      "Feature: original_glcm_ClusterProminence, Importance: 0.0021\n",
      "Feature: original_gldm_GrayLevelVariance, Importance: 0.0020\n",
      "Feature: original_ngtdm_Contrast, Importance: 0.0019\n",
      "Feature: original_gldm_LargeDependenceEmphasis, Importance: 0.0019\n",
      "Feature: original_glcm_JointEntropy, Importance: 0.0018\n",
      "Feature: original_gldm_DependenceNonUniformityNormalized, Importance: 0.0017\n",
      "Feature: original_gldm_LargeDependenceHighGrayLevelEmphasis, Importance: 0.0017\n",
      "Feature: original_glcm_Idmn, Importance: 0.0016\n",
      "Feature: original_glcm_InverseVariance, Importance: 0.0016\n",
      "Feature: original_gldm_DependenceVariance, Importance: 0.0016\n",
      "Feature: original_ngtdm_Complexity, Importance: 0.0013\n",
      "Feature: original_glrlm_RunPercentage, Importance: 0.0011\n",
      "Feature: original_glcm_Idm, Importance: 0.0011\n",
      "Feature: original_glcm_DifferenceAverage, Importance: 0.0011\n",
      "Feature: original_glcm_Contrast, Importance: 0.0010\n",
      "Feature: original_glcm_Id, Importance: 0.0008\n",
      "Feature: original_glcm_DifferenceEntropy, Importance: 0.0008\n",
      "Feature: original_firstorder_Maximum, Importance: 0.0006\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000242_003-000242_MG_BL_Series-3_Image-1-1.png', 'hcs_003-000245_003-000245_MG_BL_Series-1008_Image-1-0.png', 'hcs_003-000247_003-000247_MG_BL_Series-1005_Image-2-1.png', 'hcs_003-000252_003-000252_MG_TP3_Series-2_Image-1-1.png']\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(labelCol=\"class\", featuresCol=\"features\")\n",
    "\n",
    "# Set up parameter grid for hyperparameter tuning\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(random_forest.numTrees, [50, 150]) \\\n",
    "    .addGrid(random_forest.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(random_forest.maxBins, [32, 64, 128]) \\\n",
    "    .addGrid(random_forest.featureSubsetStrategy, ['auto', 'sqrt']) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(random_forest, file_name, rf_param_grid, is_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2261 (80.84%), Test size: 536 rows (19.16%)\n",
      "Class distribution in train_df: [0: 1130 (49.98%), 1: 1131 (50.02%)] | test_df: [0: 268 (50.00%), 1: 268 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 430 rows (19.02%) | Fold 2: 448 rows (19.81%) | Fold 3: 428 rows (18.93%) | Fold 4: 496 rows (21.94%) | Fold 5: 459 rows (20.30%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning:   0%|          | 0/15 [00:00<?, ?it/s]2024-09-23 22:37:48,620 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:37:53,579 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:37:58,946 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:38:04,021 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:38:10,937 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:38:16,541 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:38:23,041 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:38:27,822 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:38:33,094 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:38:37,968 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:   7%|▋         | 1/15 [00:54<12:45, 54.69s/it]2024-09-23 22:38:43,623 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:38:49,410 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:38:56,291 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:39:02,085 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:39:08,963 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:39:14,673 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:39:21,748 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:39:27,508 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:39:33,526 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:39:38,474 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  13%|█▎        | 2/15 [01:55<12:35, 58.09s/it]2024-09-23 22:39:44,148 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:39:49,931 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:39:57,088 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:40:02,844 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:40:09,841 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:40:15,640 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:40:22,784 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:40:28,515 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:40:35,612 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:40:41,392 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  20%|██        | 3/15 [02:59<12:10, 60.87s/it]2024-09-23 22:40:48,407 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:40:54,041 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:40:59,348 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:41:04,601 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:41:11,269 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:41:17,226 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:41:24,364 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:41:30,110 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:41:37,132 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:41:43,131 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  27%|██▋       | 4/15 [04:00<11:11, 61.07s/it]2024-09-23 22:41:49,913 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:41:56,327 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:42:03,248 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:42:09,561 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:42:16,749 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:42:22,971 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:42:30,044 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:42:36,192 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:42:43,478 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:42:49,791 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  33%|███▎      | 5/15 [05:07<10:31, 63.19s/it]2024-09-23 22:42:56,535 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:43:02,879 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:43:10,095 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:43:17,189 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:43:24,251 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:43:31,317 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:43:38,775 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:43:45,894 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:43:53,004 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:44:00,074 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  40%|████      | 6/15 [06:17<09:50, 65.56s/it]2024-09-23 22:44:07,303 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:44:13,900 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:44:21,304 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:44:27,492 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:44:34,887 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:44:41,438 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:44:48,522 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:44:54,938 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:45:02,067 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:45:08,390 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  47%|████▋     | 7/15 [07:26<08:51, 66.45s/it]2024-09-23 22:45:15,494 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:45:22,533 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:45:29,647 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:45:36,742 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:45:43,944 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:45:50,971 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:45:58,098 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:46:05,197 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:46:12,250 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:46:19,226 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  53%|█████▎    | 8/15 [08:36<07:54, 67.81s/it]2024-09-23 22:46:26,291 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:46:34,168 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:46:41,473 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:46:49,273 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:46:56,410 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:47:04,224 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:47:11,570 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:47:19,543 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:47:26,889 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:47:34,591 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  60%|██████    | 9/15 [09:52<07:01, 70.20s/it]2024-09-23 22:47:41,886 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:47:48,538 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:47:55,853 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:48:02,475 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:48:09,799 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:48:16,620 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:48:23,672 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:48:30,441 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:48:37,795 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:48:44,193 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  67%|██████▋   | 10/15 [11:02<05:50, 70.09s/it]2024-09-23 22:48:51,594 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:48:58,987 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:49:06,514 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:49:13,689 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:49:21,233 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:49:28,714 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:49:35,903 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:49:43,341 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:49:50,707 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:49:57,953 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  73%|███████▎  | 11/15 [12:15<04:44, 71.18s/it]2024-09-23 22:50:05,568 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:50:13,595 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:50:20,762 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:50:29,092 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:50:36,513 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:50:44,642 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:50:52,100 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:51:00,173 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:51:07,595 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:51:15,803 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  80%|████████  | 12/15 [13:33<03:39, 73.17s/it]2024-09-23 22:51:22,970 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:51:29,925 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:51:37,078 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:51:44,039 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:51:51,423 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:51:58,039 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:52:05,521 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:52:12,386 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:52:19,581 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:52:26,387 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  87%|████████▋ | 13/15 [14:44<02:24, 72.45s/it]2024-09-23 22:52:33,767 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:52:41,250 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:52:48,310 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:52:55,805 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:53:02,701 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:53:10,084 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:53:17,811 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:53:25,274 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:53:32,486 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:53:39,841 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  93%|█████████▎| 14/15 [15:57<01:12, 72.74s/it]2024-09-23 22:53:46,950 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:53:55,040 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:54:02,010 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:54:10,406 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:54:17,526 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:54:25,806 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:54:32,966 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:54:41,161 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 22:54:48,274 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 15, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 200}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:54:56,306 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning: 100%|██████████| 15/15 [17:14<00:00, 68.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: max_depth: 3, n_estimators: 100, learning_rate: 0.1, subsample: 0.8, colsample_bytree: 0.8\n",
      "Best areaUnderROC: 0.8818\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 22:55:01,484 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 22:55:07,207 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Evaluation on Test Data areaUnderROC: 0.9086\n",
      "Final Accuracy: 0.9086, Precision: 0.9041, Recall: 0.9142\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['hcs_003-000245_003-000245_MG_BL_Series-1008_Image-1-0.png', 'hcs_003-000247_003-000247_MG_BL_Series-1005_Image-1-1.png', 'hcs_003-000252_003-000252_MG_TP3_Series-2_Image-1-1.png', 'hcs_003-000257_003-000257_MG_BL_Series-1010_Image-3-1.png', 'hcs_003-000286_003-000286_MG_BL_Series-1010_Image-1-0.png']\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = SparkXGBClassifier(label_col=\"class\", features_col=\"features\", use_gpu=False) \n",
    "\n",
    "# Set up the parameter grid for hyperparameter tuning\n",
    "xgb_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(xgb_classifier.max_depth, [3, 6, 9, 12, 15]) \\\n",
    "    .addGrid(xgb_classifier.n_estimators, [50, 100, 200]) \\\n",
    "    .addGrid(xgb_classifier.learning_rate, [0.1]) \\\n",
    "    .addGrid(xgb_classifier.subsample, [0.8]) \\\n",
    "    .addGrid(xgb_classifier.colsample_bytree, [0.8]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(xgb_classifier, file_name, xgb_param_grid, is_tree=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2261 (80.84%), Test size: 536 rows (19.16%)\n",
      "Class distribution in train_df: [0: 1130 (49.98%), 1: 1131 (50.02%)] | test_df: [0: 268 (50.00%), 1: 268 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 430 rows (19.02%) | Fold 2: 448 rows (19.81%) | Fold 3: 428 rows (18.93%) | Fold 4: 496 rows (21.94%) | Fold 5: 459 rows (20.30%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 12/12 [06:03<00:00, 30.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: maxIter: 100, regParam: 0.01, tol: 0.0001\n",
      "Best areaUnderROC: 0.8774\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.9030\n",
      "Final Accuracy: 0.9030, Precision: 0.8750, Recall: 0.9403\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000071_001-000071_MG_TP3_Series-4_Image-1-0.png', 'hcs_003-000242_003-000242_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000242_003-000242_MG_BL_Series-4_Image-1-0.png', 'hcs_003-000245_003-000245_MG_BL_Series-1008_Image-1-0.png', 'hcs_003-000257_003-000257_MG_BL_Series-1010_Image-4-0.png']\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC with Standard Scaler\n",
    "svc = LinearSVC(labelCol=\"class\", featuresCol=\"features\")\n",
    "\n",
    "# Set up the parameter grid for hyperparameter tuning\n",
    "svc_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(svc.maxIter, [100, 500, 1000]) \\\n",
    "    .addGrid(svc.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(svc.tol, [1e-4,  1e-2]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function with the XGBoost classifier\n",
    "model = main(svc, file_name, svc_param_grid, is_tree=False, use_standard_scaler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 128 - with full mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'features_128_full_mask.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2260 (80.77%), Test size: 538 rows (19.23%)\n",
      "Class distribution in train_df: [0: 1130 (50.00%), 1: 1130 (50.00%)] | test_df: [0: 269 (50.00%), 1: 269 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 522 rows (23.10%) | Fold 2: 426 rows (18.85%) | Fold 3: 420 rows (18.58%) | Fold 4: 474 rows (20.97%) | Fold 5: 418 rows (18.50%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 8/8 [02:52<00:00, 21.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: maxDepth: 5, maxBins: 64\n",
      "Best areaUnderROC: 0.7671\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.7807\n",
      "Final Accuracy: 0.7807, Precision: 0.8159, Recall: 0.7249\n",
      "\n",
      "Sorted Feature Importances:\n",
      "Feature: original_firstorder_90Percentile, Importance: 0.7645\n",
      "Feature: original_firstorder_Variance, Importance: 0.0742\n",
      "Feature: original_gldm_LargeDependenceLowGrayLevelEmphasis, Importance: 0.0277\n",
      "Feature: original_firstorder_10Percentile, Importance: 0.0274\n",
      "Feature: original_glcm_Correlation, Importance: 0.0164\n",
      "Feature: original_firstorder_Skewness, Importance: 0.0163\n",
      "Feature: original_ngtdm_Contrast, Importance: 0.0159\n",
      "Feature: original_glrlm_GrayLevelNonUniformityNormalized, Importance: 0.0108\n",
      "Feature: original_glszm_HighGrayLevelZoneEmphasis, Importance: 0.0087\n",
      "Feature: original_firstorder_Kurtosis, Importance: 0.0084\n",
      "Feature: original_glszm_SizeZoneNonUniformity, Importance: 0.0073\n",
      "Feature: original_glrlm_HighGrayLevelRunEmphasis, Importance: 0.0068\n",
      "Feature: original_gldm_SmallDependenceHighGrayLevelEmphasis, Importance: 0.0039\n",
      "Feature: original_firstorder_MeanAbsoluteDeviation, Importance: 0.0038\n",
      "Feature: original_firstorder_RobustMeanAbsoluteDeviation, Importance: 0.0031\n",
      "Feature: original_gldm_DependenceEntropy, Importance: 0.0029\n",
      "Feature: original_glszm_ZoneEntropy, Importance: 0.0020\n",
      "Feature: original_firstorder_Energy, Importance: 0.0000\n",
      "Feature: original_firstorder_Entropy, Importance: 0.0000\n",
      "Feature: original_firstorder_InterquartileRange, Importance: 0.0000\n",
      "Feature: original_firstorder_Maximum, Importance: 0.0000\n",
      "Feature: original_firstorder_Mean, Importance: 0.0000\n",
      "Feature: original_firstorder_Median, Importance: 0.0000\n",
      "Feature: original_firstorder_Minimum, Importance: 0.0000\n",
      "Feature: original_firstorder_Range, Importance: 0.0000\n",
      "Feature: original_firstorder_RootMeanSquared, Importance: 0.0000\n",
      "Feature: original_firstorder_TotalEnergy, Importance: 0.0000\n",
      "Feature: original_firstorder_Uniformity, Importance: 0.0000\n",
      "Feature: original_glcm_Autocorrelation, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterProminence, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterShade, Importance: 0.0000\n",
      "Feature: original_glcm_ClusterTendency, Importance: 0.0000\n",
      "Feature: original_glcm_Contrast, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceAverage, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_DifferenceVariance, Importance: 0.0000\n",
      "Feature: original_glcm_Id, Importance: 0.0000\n",
      "Feature: original_glcm_Idm, Importance: 0.0000\n",
      "Feature: original_glcm_Idmn, Importance: 0.0000\n",
      "Feature: original_glcm_Idn, Importance: 0.0000\n",
      "Feature: original_glcm_Imc1, Importance: 0.0000\n",
      "Feature: original_glcm_Imc2, Importance: 0.0000\n",
      "Feature: original_glcm_InverseVariance, Importance: 0.0000\n",
      "Feature: original_glcm_JointAverage, Importance: 0.0000\n",
      "Feature: original_glcm_JointEnergy, Importance: 0.0000\n",
      "Feature: original_glcm_JointEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_MCC, Importance: 0.0000\n",
      "Feature: original_glcm_MaximumProbability, Importance: 0.0000\n",
      "Feature: original_glcm_SumAverage, Importance: 0.0000\n",
      "Feature: original_glcm_SumEntropy, Importance: 0.0000\n",
      "Feature: original_glcm_SumSquares, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceNonUniformity, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_gldm_DependenceVariance, Importance: 0.0000\n",
      "Feature: original_gldm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_gldm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_gldm_HighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LargeDependenceHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_LowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceEmphasis, Importance: 0.0000\n",
      "Feature: original_gldm_SmallDependenceLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_glrlm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LongRunLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_LowGrayLevelRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_RunEntropy, Importance: 0.0000\n",
      "Feature: original_glrlm_RunLengthNonUniformity, Importance: 0.0000\n",
      "Feature: original_glrlm_RunLengthNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glrlm_RunPercentage, Importance: 0.0000\n",
      "Feature: original_glrlm_RunVariance, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glrlm_ShortRunLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelNonUniformity, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glszm_GrayLevelVariance, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LargeAreaLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_LowGrayLevelZoneEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SizeZoneNonUniformityNormalized, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaHighGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_SmallAreaLowGrayLevelEmphasis, Importance: 0.0000\n",
      "Feature: original_glszm_ZonePercentage, Importance: 0.0000\n",
      "Feature: original_glszm_ZoneVariance, Importance: 0.0000\n",
      "Feature: original_ngtdm_Busyness, Importance: 0.0000\n",
      "Feature: original_ngtdm_Coarseness, Importance: 0.0000\n",
      "Feature: original_ngtdm_Complexity, Importance: 0.0000\n",
      "Feature: original_ngtdm_Strength, Importance: 0.0000\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-1.png', 'hcs_003-000029_003-000029_MG_BL_Series-1002_Image-1002-1.png', 'hcs_003-000029_003-000029_MG_BL_Series-1003_Image-1003-1.png', 'hcs_003-000107_003-000107_MG_BL_Series-1001_Image-1001-0.png']\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(labelCol = \"class\", featuresCol = \"features\")\n",
    "\n",
    "# Set up parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(decision_tree.maxDepth, [5, 10, 15, 20]) \\\n",
    "    .addGrid(decision_tree.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(decision_tree, file_name, param_grid, is_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2260 (80.77%), Test size: 538 rows (19.23%)\n",
      "Class distribution in train_df: [0: 1130 (50.00%), 1: 1130 (50.00%)] | test_df: [0: 269 (50.00%), 1: 269 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 522 rows (23.10%) | Fold 2: 426 rows (18.85%) | Fold 3: 420 rows (18.58%) | Fold 4: 474 rows (20.97%) | Fold 5: 418 rows (18.50%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 36/36 [34:05<00:00, 56.83s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: numTrees: 150, maxDepth: 5, maxBins: 32, featureSubsetStrategy: auto\n",
      "Best areaUnderROC: 0.7786\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.7844\n",
      "Final Accuracy: 0.7844, Precision: 0.7802, Recall: 0.7918\n",
      "\n",
      "Sorted Feature Importances:\n",
      "Feature: original_firstorder_90Percentile, Importance: 0.1049\n",
      "Feature: original_firstorder_TotalEnergy, Importance: 0.0946\n",
      "Feature: original_firstorder_Energy, Importance: 0.0918\n",
      "Feature: original_firstorder_Variance, Importance: 0.0738\n",
      "Feature: original_firstorder_Median, Importance: 0.0680\n",
      "Feature: original_firstorder_Mean, Importance: 0.0636\n",
      "Feature: original_firstorder_MeanAbsoluteDeviation, Importance: 0.0634\n",
      "Feature: original_firstorder_RootMeanSquared, Importance: 0.0628\n",
      "Feature: original_firstorder_InterquartileRange, Importance: 0.0498\n",
      "Feature: original_firstorder_RobustMeanAbsoluteDeviation, Importance: 0.0396\n",
      "Feature: original_firstorder_Skewness, Importance: 0.0311\n",
      "Feature: original_firstorder_Kurtosis, Importance: 0.0143\n",
      "Feature: original_gldm_SmallDependenceLowGrayLevelEmphasis, Importance: 0.0109\n",
      "Feature: original_glszm_LargeAreaEmphasis, Importance: 0.0103\n",
      "Feature: original_glszm_GrayLevelNonUniformity, Importance: 0.0100\n",
      "Feature: original_glszm_SizeZoneNonUniformity, Importance: 0.0082\n",
      "Feature: original_glcm_Correlation, Importance: 0.0081\n",
      "Feature: original_glszm_ZoneVariance, Importance: 0.0078\n",
      "Feature: original_glszm_ZonePercentage, Importance: 0.0076\n",
      "Feature: original_firstorder_10Percentile, Importance: 0.0071\n",
      "Feature: original_glcm_Imc1, Importance: 0.0066\n",
      "Feature: original_gldm_SmallDependenceEmphasis, Importance: 0.0060\n",
      "Feature: original_glcm_MCC, Importance: 0.0058\n",
      "Feature: original_glszm_LargeAreaLowGrayLevelEmphasis, Importance: 0.0053\n",
      "Feature: original_glrlm_LongRunEmphasis, Importance: 0.0052\n",
      "Feature: original_glszm_LowGrayLevelZoneEmphasis, Importance: 0.0043\n",
      "Feature: original_glszm_SmallAreaEmphasis, Importance: 0.0042\n",
      "Feature: original_glrlm_ShortRunLowGrayLevelEmphasis, Importance: 0.0042\n",
      "Feature: original_glszm_SmallAreaHighGrayLevelEmphasis, Importance: 0.0042\n",
      "Feature: original_glszm_LargeAreaHighGrayLevelEmphasis, Importance: 0.0038\n",
      "Feature: original_glcm_ClusterProminence, Importance: 0.0038\n",
      "Feature: original_gldm_DependenceNonUniformity, Importance: 0.0036\n",
      "Feature: original_glszm_SmallAreaLowGrayLevelEmphasis, Importance: 0.0036\n",
      "Feature: original_glcm_Imc2, Importance: 0.0036\n",
      "Feature: original_glrlm_GrayLevelNonUniformity, Importance: 0.0034\n",
      "Feature: original_glszm_HighGrayLevelZoneEmphasis, Importance: 0.0032\n",
      "Feature: original_glrlm_GrayLevelVariance, Importance: 0.0032\n",
      "Feature: original_gldm_LowGrayLevelEmphasis, Importance: 0.0031\n",
      "Feature: original_glrlm_ShortRunEmphasis, Importance: 0.0030\n",
      "Feature: original_firstorder_Uniformity, Importance: 0.0029\n",
      "Feature: original_glrlm_RunVariance, Importance: 0.0028\n",
      "Feature: original_gldm_DependenceNonUniformityNormalized, Importance: 0.0028\n",
      "Feature: original_glszm_ZoneEntropy, Importance: 0.0027\n",
      "Feature: original_glcm_SumEntropy, Importance: 0.0026\n",
      "Feature: original_glcm_ClusterTendency, Importance: 0.0026\n",
      "Feature: original_gldm_LargeDependenceEmphasis, Importance: 0.0026\n",
      "Feature: original_glrlm_HighGrayLevelRunEmphasis, Importance: 0.0025\n",
      "Feature: original_glrlm_ShortRunHighGrayLevelEmphasis, Importance: 0.0025\n",
      "Feature: original_glcm_Autocorrelation, Importance: 0.0024\n",
      "Feature: original_glrlm_LongRunHighGrayLevelEmphasis, Importance: 0.0024\n",
      "Feature: original_gldm_LargeDependenceLowGrayLevelEmphasis, Importance: 0.0023\n",
      "Feature: original_glszm_GrayLevelVariance, Importance: 0.0023\n",
      "Feature: original_gldm_LargeDependenceHighGrayLevelEmphasis, Importance: 0.0023\n",
      "Feature: original_glrlm_RunEntropy, Importance: 0.0022\n",
      "Feature: original_glrlm_LongRunLowGrayLevelEmphasis, Importance: 0.0022\n",
      "Feature: original_glszm_SizeZoneNonUniformityNormalized, Importance: 0.0022\n",
      "Feature: original_glcm_MaximumProbability, Importance: 0.0020\n",
      "Feature: original_glszm_GrayLevelNonUniformityNormalized, Importance: 0.0020\n",
      "Feature: original_glcm_ClusterShade, Importance: 0.0019\n",
      "Feature: original_glcm_SumAverage, Importance: 0.0019\n",
      "Feature: original_glcm_JointAverage, Importance: 0.0019\n",
      "Feature: original_gldm_GrayLevelVariance, Importance: 0.0019\n",
      "Feature: original_gldm_SmallDependenceHighGrayLevelEmphasis, Importance: 0.0018\n",
      "Feature: original_glcm_InverseVariance, Importance: 0.0018\n",
      "Feature: original_glrlm_RunLengthNonUniformityNormalized, Importance: 0.0018\n",
      "Feature: original_glcm_DifferenceAverage, Importance: 0.0018\n",
      "Feature: original_ngtdm_Busyness, Importance: 0.0018\n",
      "Feature: original_firstorder_Entropy, Importance: 0.0017\n",
      "Feature: original_glcm_JointEntropy, Importance: 0.0017\n",
      "Feature: original_gldm_DependenceEntropy, Importance: 0.0017\n",
      "Feature: original_glrlm_RunLengthNonUniformity, Importance: 0.0017\n",
      "Feature: original_glrlm_RunPercentage, Importance: 0.0016\n",
      "Feature: original_gldm_DependenceVariance, Importance: 0.0015\n",
      "Feature: original_gldm_GrayLevelNonUniformity, Importance: 0.0015\n",
      "Feature: original_gldm_HighGrayLevelEmphasis, Importance: 0.0015\n",
      "Feature: original_glrlm_LowGrayLevelRunEmphasis, Importance: 0.0014\n",
      "Feature: original_glcm_SumSquares, Importance: 0.0014\n",
      "Feature: original_glrlm_GrayLevelNonUniformityNormalized, Importance: 0.0014\n",
      "Feature: original_ngtdm_Contrast, Importance: 0.0013\n",
      "Feature: original_ngtdm_Complexity, Importance: 0.0011\n",
      "Feature: original_glcm_JointEnergy, Importance: 0.0011\n",
      "Feature: original_glcm_Idn, Importance: 0.0010\n",
      "Feature: original_ngtdm_Strength, Importance: 0.0010\n",
      "Feature: original_glcm_DifferenceEntropy, Importance: 0.0010\n",
      "Feature: original_glcm_DifferenceVariance, Importance: 0.0010\n",
      "Feature: original_glcm_Id, Importance: 0.0010\n",
      "Feature: original_glcm_Idmn, Importance: 0.0009\n",
      "Feature: original_ngtdm_Coarseness, Importance: 0.0007\n",
      "Feature: original_glcm_Contrast, Importance: 0.0006\n",
      "Feature: original_glcm_Idm, Importance: 0.0006\n",
      "Feature: original_firstorder_Maximum, Importance: 0.0005\n",
      "Feature: original_firstorder_Range, Importance: 0.0003\n",
      "Feature: original_firstorder_Minimum, Importance: 0.0002\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1002_Image-1002-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1003_Image-1003-0.png', 'hcs_003-000107_003-000107_MG_BL_Series-1001_Image-1001-1.png']\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(labelCol=\"class\", featuresCol=\"features\")\n",
    "\n",
    "# Set up parameter grid for hyperparameter tuning\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(random_forest.numTrees, [50, 150]) \\\n",
    "    .addGrid(random_forest.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(random_forest.maxBins, [32, 64, 128]) \\\n",
    "    .addGrid(random_forest.featureSubsetStrategy, ['auto', 'sqrt']) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(random_forest, file_name, rf_param_grid, is_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2260 (80.77%), Test size: 538 rows (19.23%)\n",
      "Class distribution in train_df: [0: 1130 (50.00%), 1: 1130 (50.00%)] | test_df: [0: 269 (50.00%), 1: 269 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 522 rows (23.10%) | Fold 2: 426 rows (18.85%) | Fold 3: 420 rows (18.58%) | Fold 4: 474 rows (20.97%) | Fold 5: 418 rows (18.50%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning:   0%|          | 0/8 [00:00<?, ?it/s]2024-09-23 23:40:20,357 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:40:25,138 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:40:30,366 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:40:35,092 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:40:40,376 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:40:45,178 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:40:50,536 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:40:55,365 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:41:01,581 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:41:07,103 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  12%|█▎        | 1/8 [00:54<06:18, 54.08s/it]2024-09-23 23:41:14,087 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:41:19,747 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:41:26,859 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:41:32,478 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:41:39,398 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:41:45,003 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:41:52,072 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:41:57,687 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:42:04,653 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:42:09,563 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  25%|██▌       | 2/8 [01:55<05:50, 58.39s/it]2024-09-23 23:42:15,482 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:42:21,446 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:42:28,515 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:42:34,458 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:42:41,574 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:42:47,471 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:42:54,676 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:43:00,694 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:43:07,848 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:43:13,877 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  38%|███▊      | 3/8 [03:00<05:08, 61.63s/it]2024-09-23 23:43:21,038 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:43:27,510 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:43:34,740 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:43:41,225 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:43:48,372 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:43:54,820 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:44:02,054 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:44:08,773 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:44:16,223 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 6, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:44:23,043 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  50%|█████     | 4/8 [04:10<04:18, 64.62s/it]2024-09-23 23:44:30,343 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:44:37,193 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:44:44,687 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:44:51,453 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:44:58,939 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:45:05,831 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:45:13,299 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:45:20,175 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:45:27,590 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:45:34,479 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  62%|██████▎   | 5/8 [05:21<03:21, 67.06s/it]2024-09-23 23:45:41,898 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:45:49,663 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:45:56,966 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:46:04,679 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:46:12,010 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:46:19,826 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:46:27,278 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:46:34,899 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:46:42,221 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 9, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:46:49,768 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  75%|███████▌  | 6/8 [06:36<02:19, 69.89s/it]2024-09-23 23:46:57,203 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:47:04,610 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:47:12,107 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:47:19,663 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:47:27,043 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:47:34,323 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:47:41,627 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:47:48,940 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:47:56,258 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 50}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:48:03,679 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning:  88%|████████▊ | 7/8 [07:50<01:11, 71.24s/it]2024-09-23 23:48:11,096 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:48:19,426 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:48:26,793 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:48:35,895 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:48:43,598 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:48:51,848 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:48:59,014 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:49:07,458 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "2024-09-23 23:49:14,832 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 12, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:49:23,176 INFO XGBoost-PySpark: _fit Finished xgboost training!\n",
      "Hyperparameter Tuning: 100%|██████████| 8/8 [09:10<00:00, 68.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: max_depth: 3, n_estimators: 100, learning_rate: 0.1, subsample: 0.8, colsample_bytree: 0.8\n",
      "Best areaUnderROC: 0.7831\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:49:28,211 INFO XGBoost-PySpark: _fit Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'colsample_bytree': 0.8, 'device': 'cpu', 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'binary:logistic', 'subsample': 0.8, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2024-09-23 23:49:34,141 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Evaluation on Test Data areaUnderROC: 0.7862\n",
      "Final Accuracy: 0.7862, Precision: 0.7852, Recall: 0.7881\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1002_Image-1002-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1002_Image-1002-1.png', 'hcs_003-000029_003-000029_MG_BL_Series-1003_Image-1003-0.png']\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = SparkXGBClassifier(label_col=\"class\", features_col=\"features\", use_gpu=False) \n",
    "\n",
    "# Set up the parameter grid for hyperparameter tuning\n",
    "xgb_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(xgb_classifier.max_depth, [3, 6, 9, 12]) \\\n",
    "    .addGrid(xgb_classifier.n_estimators, [50, 100]) \\\n",
    "    .addGrid(xgb_classifier.learning_rate, [0.1]) \\\n",
    "    .addGrid(xgb_classifier.subsample, [0.8]) \\\n",
    "    .addGrid(xgb_classifier.colsample_bytree, [0.8]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function to train and evaluate the model\n",
    "model = main(xgb_classifier, file_name, xgb_param_grid, is_tree=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      " Setting up\n",
      "===========================================================================\n",
      "\n",
      "No overlapping patients between training and test sets.\n",
      "Training size: 2260 (80.77%), Test size: 538 rows (19.23%)\n",
      "Class distribution in train_df: [0: 1130 (50.00%), 1: 1130 (50.00%)] | test_df: [0: 269 (50.00%), 1: 269 (50.00%)]\n",
      "\n",
      "===========================================================================\n",
      " Hyperparameter tunning\n",
      "===========================================================================\n",
      "\n",
      "Fold 1: 522 rows (23.10%) | Fold 2: 426 rows (18.85%) | Fold 3: 420 rows (18.58%) | Fold 4: 474 rows (20.97%) | Fold 5: 418 rows (18.50%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|██████████| 12/12 [06:15<00:00, 31.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Parameters: maxIter: 100, regParam: 0.01, tol: 0.0001\n",
      "Best areaUnderROC: 0.7775\n",
      "\n",
      "===========================================================================\n",
      " Testing model on test dataset\n",
      "===========================================================================\n",
      "\n",
      "Number of initial columns: 119, number of feature columns: 93\n",
      "Final Model Evaluation on Test Data areaUnderROC: 0.8030\n",
      "Final Accuracy: 0.8030, Precision: 0.7860, Recall: 0.8327\n",
      "\n",
      "First 5 names of wrongly classified rows:\n",
      "['auth_001-000084_001-000084_MG_BL_Series-3_Image-1-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1001_Image-1001-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1002_Image-1002-0.png', 'hcs_003-000029_003-000029_MG_BL_Series-1003_Image-1003-0.png', 'hcs_003-000107_003-000107_MG_BL_Series-1001_Image-1001-1.png']\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC with Standard Scaler\n",
    "svc = LinearSVC(labelCol=\"class\", featuresCol=\"features\")\n",
    "\n",
    "# Set up the parameter grid for hyperparameter tuning\n",
    "svc_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(svc.maxIter, [100, 500, 1000]) \\\n",
    "    .addGrid(svc.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(svc.tol, [1e-4,  1e-2]) \\\n",
    "    .build()\n",
    "\n",
    "# Call the main function with the XGBoost classifier\n",
    "model = main(svc, file_name, svc_param_grid, is_tree=False, use_standard_scaler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
